{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_file=\"../pretrained/bert-base-uncased-pytorch_model.bin\"\n",
    "# archive_file=\"bert-base-uncased-pytorch_model.bin\"\n",
    "pretrain_state_dict = torch.load(pretrain_file, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "torch.Size([30522, 768])\n",
      "bert.embeddings.position_embeddings.weight\n",
      "torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.embeddings.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.pooler.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.pooler.dense.bias\n",
      "torch.Size([768])\n",
      "cls.predictions.bias\n",
      "torch.Size([30522])\n",
      "cls.predictions.transform.dense.weight\n",
      "torch.Size([768, 768])\n",
      "cls.predictions.transform.dense.bias\n",
      "torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "cls.predictions.decoder.weight\n",
      "torch.Size([30522, 768])\n",
      "cls.seq_relationship.weight\n",
      "torch.Size([2, 768])\n",
      "cls.seq_relationship.bias\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# for k in state_dict.keys():\n",
    "#     print(k)\n",
    "#     print(state_dict[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import truncnorm\n",
    "def truncated_normal(size, threshold=1):\n",
    "    values = truncnorm.rvs(-threshold, threshold, size=size)\n",
    "    x = torch.from_numpy(values)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinitialize_weights():\n",
    "    REINITIALIZE_WEIGHTS_FILE = \"reinitialize_weights_layer10_layer11.bin\"\n",
    "    pretrain_file=\"../pretrained/bert-base-uncased-pytorch_model.bin\"\n",
    "    pretrain_state_dict = torch.load(pretrain_file, map_location=\"cpu\")\n",
    "    \n",
    "    state_dict = {}\n",
    "    for k in pretrain_state_dict.keys():\n",
    "        if '.10.' in k or '.11.' in k:\n",
    "            print('REINITIALIZED: ', k)\n",
    "            parameters = truncated_normal(pretrain_state_dict[k].shape)\n",
    "            state_dict[k] = parameters\n",
    "        else:\n",
    "            print('PRETAIN: ', k)\n",
    "            state_dict[k] = pretrain_state_dict[k]\n",
    "        \n",
    "    torch.save(state_dict, REINITIALIZE_WEIGHTS_FILE)\n",
    "    print(\"saved reinitialized weights: \", REINITIALIZE_WEIGHTS_FILE)\n",
    "    \n",
    "    return REINITIALIZE_WEIGHTS_FILE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRETAIN:  bert.embeddings.word_embeddings.weight\n",
      "PRETAIN:  bert.embeddings.position_embeddings.weight\n",
      "PRETAIN:  bert.embeddings.token_type_embeddings.weight\n",
      "PRETAIN:  bert.embeddings.LayerNorm.gamma\n",
      "PRETAIN:  bert.embeddings.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.0.attention.self.query.weight\n",
      "PRETAIN:  bert.encoder.layer.0.attention.self.query.bias\n",
      "PRETAIN:  bert.encoder.layer.0.attention.self.key.weight\n",
      "PRETAIN:  bert.encoder.layer.0.attention.self.key.bias\n",
      "PRETAIN:  bert.encoder.layer.0.attention.self.value.weight\n",
      "PRETAIN:  bert.encoder.layer.0.attention.self.value.bias\n",
      "PRETAIN:  bert.encoder.layer.0.attention.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.0.attention.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.0.attention.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.0.attention.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.0.intermediate.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.0.intermediate.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.0.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.0.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.0.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.0.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.1.attention.self.query.weight\n",
      "PRETAIN:  bert.encoder.layer.1.attention.self.query.bias\n",
      "PRETAIN:  bert.encoder.layer.1.attention.self.key.weight\n",
      "PRETAIN:  bert.encoder.layer.1.attention.self.key.bias\n",
      "PRETAIN:  bert.encoder.layer.1.attention.self.value.weight\n",
      "PRETAIN:  bert.encoder.layer.1.attention.self.value.bias\n",
      "PRETAIN:  bert.encoder.layer.1.attention.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.1.attention.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.1.attention.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.1.attention.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.1.intermediate.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.1.intermediate.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.1.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.1.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.1.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.1.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.2.attention.self.query.weight\n",
      "PRETAIN:  bert.encoder.layer.2.attention.self.query.bias\n",
      "PRETAIN:  bert.encoder.layer.2.attention.self.key.weight\n",
      "PRETAIN:  bert.encoder.layer.2.attention.self.key.bias\n",
      "PRETAIN:  bert.encoder.layer.2.attention.self.value.weight\n",
      "PRETAIN:  bert.encoder.layer.2.attention.self.value.bias\n",
      "PRETAIN:  bert.encoder.layer.2.attention.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.2.attention.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.2.attention.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.2.attention.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.2.intermediate.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.2.intermediate.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.2.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.2.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.2.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.2.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.3.attention.self.query.weight\n",
      "PRETAIN:  bert.encoder.layer.3.attention.self.query.bias\n",
      "PRETAIN:  bert.encoder.layer.3.attention.self.key.weight\n",
      "PRETAIN:  bert.encoder.layer.3.attention.self.key.bias\n",
      "PRETAIN:  bert.encoder.layer.3.attention.self.value.weight\n",
      "PRETAIN:  bert.encoder.layer.3.attention.self.value.bias\n",
      "PRETAIN:  bert.encoder.layer.3.attention.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.3.attention.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.3.attention.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.3.attention.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.3.intermediate.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.3.intermediate.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.3.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.3.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.3.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.3.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.4.attention.self.query.weight\n",
      "PRETAIN:  bert.encoder.layer.4.attention.self.query.bias\n",
      "PRETAIN:  bert.encoder.layer.4.attention.self.key.weight\n",
      "PRETAIN:  bert.encoder.layer.4.attention.self.key.bias\n",
      "PRETAIN:  bert.encoder.layer.4.attention.self.value.weight\n",
      "PRETAIN:  bert.encoder.layer.4.attention.self.value.bias\n",
      "PRETAIN:  bert.encoder.layer.4.attention.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.4.attention.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.4.attention.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.4.attention.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.4.intermediate.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.4.intermediate.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.4.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.4.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.4.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.4.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.5.attention.self.query.weight\n",
      "PRETAIN:  bert.encoder.layer.5.attention.self.query.bias\n",
      "PRETAIN:  bert.encoder.layer.5.attention.self.key.weight\n",
      "PRETAIN:  bert.encoder.layer.5.attention.self.key.bias\n",
      "PRETAIN:  bert.encoder.layer.5.attention.self.value.weight\n",
      "PRETAIN:  bert.encoder.layer.5.attention.self.value.bias\n",
      "PRETAIN:  bert.encoder.layer.5.attention.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.5.attention.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.5.attention.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.5.attention.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.5.intermediate.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.5.intermediate.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.5.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.5.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.5.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.5.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.6.attention.self.query.weight\n",
      "PRETAIN:  bert.encoder.layer.6.attention.self.query.bias\n",
      "PRETAIN:  bert.encoder.layer.6.attention.self.key.weight\n",
      "PRETAIN:  bert.encoder.layer.6.attention.self.key.bias\n",
      "PRETAIN:  bert.encoder.layer.6.attention.self.value.weight\n",
      "PRETAIN:  bert.encoder.layer.6.attention.self.value.bias\n",
      "PRETAIN:  bert.encoder.layer.6.attention.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.6.attention.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.6.attention.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.6.attention.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.6.intermediate.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.6.intermediate.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.6.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.6.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.6.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.6.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.7.attention.self.query.weight\n",
      "PRETAIN:  bert.encoder.layer.7.attention.self.query.bias\n",
      "PRETAIN:  bert.encoder.layer.7.attention.self.key.weight\n",
      "PRETAIN:  bert.encoder.layer.7.attention.self.key.bias\n",
      "PRETAIN:  bert.encoder.layer.7.attention.self.value.weight\n",
      "PRETAIN:  bert.encoder.layer.7.attention.self.value.bias\n",
      "PRETAIN:  bert.encoder.layer.7.attention.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.7.attention.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.7.attention.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.7.attention.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.7.intermediate.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.7.intermediate.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.7.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.7.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.7.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.7.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.8.attention.self.query.weight\n",
      "PRETAIN:  bert.encoder.layer.8.attention.self.query.bias\n",
      "PRETAIN:  bert.encoder.layer.8.attention.self.key.weight\n",
      "PRETAIN:  bert.encoder.layer.8.attention.self.key.bias\n",
      "PRETAIN:  bert.encoder.layer.8.attention.self.value.weight\n",
      "PRETAIN:  bert.encoder.layer.8.attention.self.value.bias\n",
      "PRETAIN:  bert.encoder.layer.8.attention.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.8.attention.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.8.attention.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.8.attention.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.8.intermediate.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.8.intermediate.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.8.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.8.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.8.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.8.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.9.attention.self.query.weight\n",
      "PRETAIN:  bert.encoder.layer.9.attention.self.query.bias\n",
      "PRETAIN:  bert.encoder.layer.9.attention.self.key.weight\n",
      "PRETAIN:  bert.encoder.layer.9.attention.self.key.bias\n",
      "PRETAIN:  bert.encoder.layer.9.attention.self.value.weight\n",
      "PRETAIN:  bert.encoder.layer.9.attention.self.value.bias\n",
      "PRETAIN:  bert.encoder.layer.9.attention.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.9.attention.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.9.attention.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.9.attention.output.LayerNorm.beta\n",
      "PRETAIN:  bert.encoder.layer.9.intermediate.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.9.intermediate.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.9.output.dense.weight\n",
      "PRETAIN:  bert.encoder.layer.9.output.dense.bias\n",
      "PRETAIN:  bert.encoder.layer.9.output.LayerNorm.gamma\n",
      "PRETAIN:  bert.encoder.layer.9.output.LayerNorm.beta\n",
      "REINITIALIZED:  bert.encoder.layer.10.attention.self.query.weight\n",
      "REINITIALIZED:  bert.encoder.layer.10.attention.self.query.bias\n",
      "REINITIALIZED:  bert.encoder.layer.10.attention.self.key.weight\n",
      "REINITIALIZED:  bert.encoder.layer.10.attention.self.key.bias\n",
      "REINITIALIZED:  bert.encoder.layer.10.attention.self.value.weight\n",
      "REINITIALIZED:  bert.encoder.layer.10.attention.self.value.bias\n",
      "REINITIALIZED:  bert.encoder.layer.10.attention.output.dense.weight\n",
      "REINITIALIZED:  bert.encoder.layer.10.attention.output.dense.bias\n",
      "REINITIALIZED:  bert.encoder.layer.10.attention.output.LayerNorm.gamma\n",
      "REINITIALIZED:  bert.encoder.layer.10.attention.output.LayerNorm.beta\n",
      "REINITIALIZED:  bert.encoder.layer.10.intermediate.dense.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REINITIALIZED:  bert.encoder.layer.10.intermediate.dense.bias\n",
      "REINITIALIZED:  bert.encoder.layer.10.output.dense.weight\n",
      "REINITIALIZED:  bert.encoder.layer.10.output.dense.bias\n",
      "REINITIALIZED:  bert.encoder.layer.10.output.LayerNorm.gamma\n",
      "REINITIALIZED:  bert.encoder.layer.10.output.LayerNorm.beta\n",
      "REINITIALIZED:  bert.encoder.layer.11.attention.self.query.weight\n",
      "REINITIALIZED:  bert.encoder.layer.11.attention.self.query.bias\n",
      "REINITIALIZED:  bert.encoder.layer.11.attention.self.key.weight\n",
      "REINITIALIZED:  bert.encoder.layer.11.attention.self.key.bias\n",
      "REINITIALIZED:  bert.encoder.layer.11.attention.self.value.weight\n",
      "REINITIALIZED:  bert.encoder.layer.11.attention.self.value.bias\n",
      "REINITIALIZED:  bert.encoder.layer.11.attention.output.dense.weight\n",
      "REINITIALIZED:  bert.encoder.layer.11.attention.output.dense.bias\n",
      "REINITIALIZED:  bert.encoder.layer.11.attention.output.LayerNorm.gamma\n",
      "REINITIALIZED:  bert.encoder.layer.11.attention.output.LayerNorm.beta\n",
      "REINITIALIZED:  bert.encoder.layer.11.intermediate.dense.weight\n",
      "REINITIALIZED:  bert.encoder.layer.11.intermediate.dense.bias\n",
      "REINITIALIZED:  bert.encoder.layer.11.output.dense.weight\n",
      "REINITIALIZED:  bert.encoder.layer.11.output.dense.bias\n",
      "REINITIALIZED:  bert.encoder.layer.11.output.LayerNorm.gamma\n",
      "REINITIALIZED:  bert.encoder.layer.11.output.LayerNorm.beta\n",
      "PRETAIN:  bert.pooler.dense.weight\n",
      "PRETAIN:  bert.pooler.dense.bias\n",
      "PRETAIN:  cls.predictions.bias\n",
      "PRETAIN:  cls.predictions.transform.dense.weight\n",
      "PRETAIN:  cls.predictions.transform.dense.bias\n",
      "PRETAIN:  cls.predictions.transform.LayerNorm.gamma\n",
      "PRETAIN:  cls.predictions.transform.LayerNorm.beta\n",
      "PRETAIN:  cls.predictions.decoder.weight\n",
      "PRETAIN:  cls.seq_relationship.weight\n",
      "PRETAIN:  cls.seq_relationship.bias\n",
      "saved reinitialized weights:  reinitialize_weights_layer10_layer11.bin\n"
     ]
    }
   ],
   "source": [
    "reinitialize_weight_file = reinitialize_weights()\n",
    "state_dict = torch.load(reinitialize_weight_file, map_location=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "torch.Size([30522, 768])\n",
      "bert.embeddings.position_embeddings.weight\n",
      "torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "torch.Size([2, 768])\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.pooler.dense.weight\n",
      "torch.Size([768, 768])\n",
      "cls.predictions.transform.dense.weight\n",
      "torch.Size([768, 768])\n",
      "cls.predictions.decoder.weight\n",
      "torch.Size([30522, 768])\n",
      "cls.seq_relationship.weight\n",
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# total = 0 \n",
    "# for k in state_dict.keys():\n",
    "#     if 'weight' in k:\n",
    "#         print(k)\n",
    "#         total += 1 \n",
    "    \n",
    "#         print(state_dict[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(self_generated_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0392,  0.7940,  0.5212, -0.8194, -0.0927, -0.1141, -0.1154, -0.8871,\n",
       "         0.1155, -0.0987, -0.9839, -0.0524, -0.3792,  0.3771, -0.0448, -0.9734,\n",
       "        -0.9338, -0.5460,  0.4948,  0.5826, -0.4724,  0.4210,  0.1925, -0.3941,\n",
       "        -0.8376, -0.1763, -0.9071,  0.5935, -0.1801,  0.7270, -0.9954,  0.6480,\n",
       "         0.3242, -0.7355,  0.0643, -0.2359,  0.7454, -0.2562, -0.3526, -0.5003,\n",
       "         0.8909, -0.7294, -0.4778,  0.7649, -0.2613, -0.3540, -0.3341, -0.3892,\n",
       "         0.4454,  0.1995, -0.6513, -0.0082, -0.8064, -0.1140, -0.3329, -0.4113,\n",
       "         0.9518, -0.5713, -0.4802,  0.0392, -0.6485, -0.7797,  0.2227,  0.4778,\n",
       "        -0.3169,  0.4430,  0.1516, -0.6602, -0.5672, -0.8043,  0.9785, -0.6142,\n",
       "        -0.0452, -0.5278,  0.0739,  0.3931, -0.5565,  0.4845,  0.1360, -0.7983,\n",
       "         0.2009, -0.1162, -0.0600, -0.1309,  0.6377,  0.4716,  0.3676,  0.3032,\n",
       "        -0.2112, -0.8685,  0.0158, -0.0412, -0.2666, -0.6967,  0.5650,  0.6677,\n",
       "         0.0451,  0.2603, -0.8925, -0.0322, -0.0883,  0.6725, -0.6398,  0.6027,\n",
       "        -0.6636, -0.1261,  0.7989,  0.3623,  0.8975, -0.9529, -0.4624, -0.9711,\n",
       "         0.4639, -0.3715, -0.3489,  0.0516, -0.6155,  0.6638, -0.3465,  0.0919,\n",
       "         0.9789, -0.0955, -0.0226, -0.7630,  0.2244,  0.6045, -0.2383,  0.2019,\n",
       "        -0.4522,  0.1131,  0.0297,  0.8519, -0.4211, -0.9908,  0.7863,  0.8633,\n",
       "         0.8697,  0.5977, -0.0938, -0.7730,  0.5958, -0.5219,  0.6551,  0.1703,\n",
       "        -0.2286, -0.2978, -0.0164, -0.8986, -0.6636,  0.3314,  0.6779, -0.9735,\n",
       "         0.4630, -0.8037,  0.3466,  0.7999,  0.7402,  0.5425,  0.0503, -0.6581,\n",
       "         0.3753,  0.1927,  0.5317,  0.3210, -0.5137,  0.2177, -0.2059, -0.7475,\n",
       "         0.6760, -0.3530,  0.0212, -0.0563, -0.0590, -0.3309,  0.5555, -0.9135,\n",
       "        -0.0336, -0.6726,  0.5007, -0.2904,  0.0559, -0.1073,  0.0618, -0.6845,\n",
       "        -0.6188, -0.7370, -0.2521,  0.4015,  0.3663, -0.8284, -0.5516, -0.2701,\n",
       "        -0.8888,  0.5294,  0.2740,  0.0245,  0.0362, -0.7607, -0.5615,  0.8735,\n",
       "         0.0093,  0.4710,  0.1211, -0.6361, -0.2486, -0.5287,  0.1591, -0.3557,\n",
       "        -0.1013,  0.5207, -0.5519,  0.3695,  0.9212, -0.9318, -0.5796, -0.0324,\n",
       "        -0.8031, -0.1255, -0.5927, -0.6139, -0.8750, -0.1346, -0.0763,  0.1857,\n",
       "         0.2807, -0.5712, -0.1405, -0.3256, -0.5984,  0.8089, -0.7076, -0.0907,\n",
       "         0.3564,  0.3579, -0.6417, -0.7898, -0.7358,  0.6538,  0.3072, -0.2601,\n",
       "        -0.9206,  0.7445, -0.7170, -0.8091,  0.5509,  0.3584,  0.3021,  0.6329,\n",
       "         0.5468,  0.1908, -0.8118, -0.2312,  0.7015,  0.3899, -0.9519, -0.4054,\n",
       "         0.2268, -0.8980,  0.4613,  0.3131, -0.0078,  0.2151, -0.5208, -0.5543,\n",
       "         0.4642, -0.9325, -0.1793, -0.1771, -0.8496,  0.3332,  0.3780,  0.8994,\n",
       "         0.6697, -0.9677,  0.8936, -0.0263, -0.3613,  0.4717,  0.5472, -0.2700,\n",
       "        -0.1644,  0.3828, -0.8248, -0.1657, -0.1045, -0.4277,  0.6411, -0.9768,\n",
       "        -0.3919,  0.0328,  0.3673,  0.7830, -0.8898, -0.6587, -0.3186, -0.4307,\n",
       "        -0.2757,  0.6993, -0.9408,  0.3998, -0.7014,  0.7779, -0.0563, -0.2516,\n",
       "        -0.7660, -0.6605, -0.2741,  0.1217,  0.5206, -0.7331,  0.2448, -0.8320,\n",
       "         0.4271,  0.6449, -0.2884, -0.0018, -0.2155, -0.0991, -0.5850,  0.2683,\n",
       "        -0.7996, -0.3435,  0.0368,  0.6172, -0.1528, -0.1054,  0.2854,  0.7591,\n",
       "         0.3289,  0.3235,  0.6606,  0.8377, -0.5468,  0.3643,  0.8125,  0.0684,\n",
       "         0.5213, -0.7717,  0.2277,  0.4666,  0.1826,  0.4164,  0.2816,  0.1638,\n",
       "         0.2028, -0.1581,  0.0040,  0.0844, -0.3635,  0.2989,  0.0659,  0.4209,\n",
       "         0.2321, -0.5586, -0.5947, -0.2387, -0.5775, -0.1405, -0.3480,  0.0194,\n",
       "         0.9933,  0.8529,  0.3699, -0.3871,  0.7267,  0.6676,  0.6966, -0.3447,\n",
       "        -0.7711,  0.3898,  0.7445,  0.3234,  0.3776, -0.5335, -0.4222, -0.0737,\n",
       "         0.5126,  0.6815, -0.8038,  0.2311, -0.7783, -0.7632, -0.6393,  0.2935,\n",
       "        -0.9039,  0.1459, -0.4105, -0.1644, -0.8105,  0.5458,  0.4477, -0.0345,\n",
       "         0.1029, -0.3764,  0.9554,  0.0959,  0.4737,  0.5910, -0.3790, -0.5386,\n",
       "        -0.1892,  0.5825, -0.4660,  0.2323, -0.8979,  0.3177,  0.5974, -0.7982,\n",
       "        -0.3178, -0.6722,  0.4419,  0.4815, -0.5420, -0.1720,  0.7527, -0.1876,\n",
       "         0.8969,  0.9141,  0.1937, -0.5915, -0.3233, -0.5475,  0.2429, -0.1633,\n",
       "        -0.3203, -0.6041,  0.7087,  0.2842, -0.3672,  0.2929,  0.0669,  0.6545,\n",
       "         0.2747,  0.6366, -0.1369, -0.6119, -0.4770, -0.4138, -0.9192, -0.7774,\n",
       "         0.3126,  0.0484,  0.4412, -0.1062,  0.4112,  0.8173,  0.3485, -0.3205,\n",
       "        -0.8551,  0.2441,  0.3886, -0.7048,  0.5972,  0.6959, -0.1648,  0.6942,\n",
       "         0.9164, -0.1798,  0.2117,  0.9240, -0.7832,  0.7361, -0.9772,  0.9791,\n",
       "        -0.8079, -0.6609,  0.8025,  0.1593,  0.0394,  0.5202,  0.2641, -0.1503,\n",
       "         0.9973, -0.4411, -0.0924,  0.7889,  0.3569,  0.5061, -0.1345, -0.3627,\n",
       "         0.0362,  0.2057,  0.8062, -0.5226, -0.3397,  0.7418,  0.2681,  0.5015,\n",
       "        -0.9183,  0.0556,  0.2835, -0.6192,  0.9820, -0.8484,  0.9994, -0.5506,\n",
       "        -0.4114,  0.5802, -0.3647,  0.2888, -0.8884,  0.4925, -0.4265,  0.2523,\n",
       "        -0.0657,  0.7021,  0.8327, -0.0582, -0.4972, -0.7416, -0.5653, -0.1288,\n",
       "         0.8843, -0.6150,  0.2568,  0.9333,  0.2538,  0.7421, -0.3815,  0.1683,\n",
       "         0.6453, -0.0236,  0.8356,  0.0788,  0.9578,  0.3636, -0.4993, -0.0123,\n",
       "         0.3271, -0.6249, -0.3293,  0.0299, -0.5856,  0.5101,  0.6409, -0.0281,\n",
       "        -0.6206,  0.5184, -0.5588, -0.8650,  0.2134,  0.7747, -0.2045,  0.2257,\n",
       "         0.7035, -0.9020, -0.6723, -0.6946,  0.2937, -0.2134,  0.2107, -0.7059,\n",
       "         0.1713, -0.4865,  0.7775,  0.9565,  0.5103,  0.6410,  0.2017,  0.3924,\n",
       "         0.6884,  0.8261, -0.5354,  0.1772,  0.5597, -0.5699,  0.2457, -0.9458,\n",
       "         0.3140, -0.9946,  0.4794, -0.1138, -0.4985,  0.0395, -0.0660, -0.3160,\n",
       "        -0.3584,  0.6240, -0.1514, -0.7824, -0.1615, -0.6246,  0.4620,  0.7179,\n",
       "        -0.8096,  0.2925,  0.7521,  0.3272, -0.0837,  0.6379, -0.4921, -0.4794,\n",
       "        -0.9601, -0.9155,  0.8540,  0.3636,  0.3262, -0.4339,  0.7622,  0.7831,\n",
       "        -0.9474, -0.1705,  0.4614, -0.6780,  0.5501,  0.1950,  0.0303, -0.8649,\n",
       "         0.2322,  0.2444, -0.5093,  0.4395,  0.6058, -0.8769, -0.8647, -0.4232,\n",
       "        -0.2660, -0.5765,  0.7208, -0.6672,  0.5326,  0.1895, -0.3197, -0.3241,\n",
       "        -0.7793, -0.7496, -0.2228,  0.9277,  0.0204, -0.9588, -0.0644,  0.9105,\n",
       "         0.1978, -0.0897,  0.3110, -0.2831,  0.3039, -0.4154, -0.7452, -0.0803,\n",
       "        -0.2481, -0.9113,  0.7906, -0.3306, -0.0694,  0.0156, -0.8875,  0.4236,\n",
       "         0.2830, -0.5799,  0.6989, -0.4911,  0.7550,  0.7651, -0.7493,  0.9913,\n",
       "         0.7641,  0.1518, -0.3331, -0.9337,  0.4940,  0.6338,  0.5508, -0.4681,\n",
       "         0.7359,  0.3845,  0.2055, -0.7895,  0.1971,  0.3778, -0.5636,  0.8274,\n",
       "         0.6614,  0.8897, -0.5829,  0.4761, -0.4993,  0.1302, -0.6770, -0.0322,\n",
       "         0.4853, -0.6112,  0.0435,  0.1927, -0.7399,  0.2233,  0.6902, -0.0944,\n",
       "         0.3240,  0.4891, -0.0639,  0.0819, -0.7057, -0.9417,  0.2363, -0.6610,\n",
       "         0.0130, -0.8829, -0.7940, -0.0245, -0.2107, -0.7224, -0.6452,  0.0952,\n",
       "        -0.2418, -0.0014, -0.2784,  0.1101, -0.5068, -0.4403,  0.4487,  0.3749,\n",
       "        -0.8726,  0.1942,  0.0701, -0.2274, -0.5598,  0.2869,  0.0530,  0.3345,\n",
       "        -0.8732,  0.8658, -0.7011,  0.9116, -0.6012,  0.0720,  0.8175, -0.3574,\n",
       "        -0.6164,  0.1505,  0.6040,  0.7714, -0.7796, -0.8513, -0.0445,  0.2068,\n",
       "        -0.0286,  0.4771, -0.4009, -0.7955,  0.6722,  0.8654, -0.3183,  0.0047,\n",
       "         0.6764,  0.5051, -0.4091,  0.0796,  0.2000,  0.1915, -0.5160, -0.5218,\n",
       "        -0.4329,  0.1422,  0.4943, -0.3734, -0.5956, -0.3075, -0.2340,  0.3749,\n",
       "         0.8777,  0.0748,  0.5227,  0.4174, -0.6698, -0.8281,  0.4980, -0.5405],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_generated_dict['bert.encoder.layer.0.attention.self.query.weight'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.0774e-03, -5.5992e-02,  3.8239e-02, -3.3233e-02,  2.1458e-02,\n",
       "        -6.4455e-02,  5.4764e-02,  4.3823e-02, -1.6064e-02,  1.8921e-02,\n",
       "        -8.6033e-02,  4.8640e-02, -2.3422e-02, -6.5712e-02,  4.0450e-02,\n",
       "         3.5652e-02, -3.4220e-02,  2.7226e-02,  2.5244e-02, -5.0143e-02,\n",
       "         2.8682e-02, -1.5413e-02,  2.7495e-02, -3.6229e-02,  7.7442e-02,\n",
       "        -4.8504e-02,  4.9910e-02, -3.6341e-02,  3.7231e-02, -7.6122e-02,\n",
       "        -6.0755e-02, -8.4336e-02, -2.6144e-03, -1.7555e-02, -8.8574e-03,\n",
       "         4.6355e-03,  6.5866e-03,  1.0598e-02, -1.4730e-02, -5.4885e-02,\n",
       "        -2.4584e-02, -3.4590e-02,  2.8554e-02, -3.4476e-03,  2.1922e-02,\n",
       "        -2.2579e-02, -1.4900e-02, -6.3395e-02, -2.5169e-02, -2.4746e-03,\n",
       "         3.9507e-02,  2.3694e-02, -1.8066e-02, -1.2348e-01,  2.9972e-03,\n",
       "         7.3432e-03,  7.0596e-03,  1.2542e-02,  9.1366e-03,  6.7050e-02,\n",
       "        -1.6060e-03, -4.7604e-02, -5.5195e-02,  2.5834e-02,  2.1503e-02,\n",
       "         1.3148e-02, -6.9125e-03, -1.9881e-02,  1.8151e-02, -4.8156e-02,\n",
       "        -2.6136e-02, -6.6927e-03,  1.2630e-02, -7.4166e-03, -4.2118e-02,\n",
       "        -1.9636e-02, -4.3284e-02, -4.4879e-02, -7.3620e-03,  5.6118e-03,\n",
       "         2.1208e-02,  2.0403e-02, -1.3219e-02,  1.0566e-02,  3.2724e-03,\n",
       "         1.9123e-02,  5.0749e-02, -2.6302e-02, -1.4209e-02, -5.1740e-02,\n",
       "        -6.3863e-02,  3.0032e-02, -7.5452e-02,  2.1448e-02, -5.6118e-02,\n",
       "        -1.9573e-04, -3.3836e-02,  3.2877e-02, -4.2166e-02,  4.6934e-02,\n",
       "         1.7253e-02, -2.3869e-02, -3.7186e-02, -6.5244e-02, -1.0908e-02,\n",
       "        -1.9583e-02, -1.0895e-02,  3.2845e-02, -4.5613e-03, -3.0068e-02,\n",
       "         8.3320e-03, -4.7294e-02, -3.5751e-02,  4.5909e-03,  1.4962e-02,\n",
       "        -6.0977e-02,  1.4837e-02,  3.3025e-02, -4.4838e-02,  2.1898e-02,\n",
       "        -3.3346e-03,  9.2804e-02, -4.5945e-02, -9.6911e-03,  3.7121e-02,\n",
       "         3.0822e-02,  7.7563e-02,  1.9904e-02,  6.9034e-03,  3.1159e-02,\n",
       "         2.5238e-02,  1.0472e-02,  4.2969e-02,  2.9317e-03,  9.4820e-03,\n",
       "        -2.8842e-02,  8.2649e-02,  1.5709e-02,  1.0576e-02, -4.3460e-02,\n",
       "        -2.6077e-02, -9.3104e-02,  1.1650e-02, -2.6966e-02, -6.2419e-03,\n",
       "        -3.1481e-03,  4.6602e-03,  4.7172e-02, -2.2765e-02,  1.6794e-02,\n",
       "         4.4680e-02,  6.2562e-02,  7.7261e-04, -3.3720e-02, -2.4541e-02,\n",
       "         4.4200e-02, -3.6203e-02, -4.3246e-02, -1.2652e-02,  8.5492e-02,\n",
       "        -4.2757e-02,  1.5221e-02,  3.0281e-02, -8.5629e-03, -7.7063e-02,\n",
       "         7.3060e-02, -1.0537e-02, -3.2438e-02, -2.8271e-02,  1.0515e-02,\n",
       "         3.5941e-02,  5.7693e-02, -1.7827e-02,  4.0907e-02, -3.1541e-02,\n",
       "        -2.6617e-02,  1.0661e-03, -5.6058e-03, -3.5773e-02,  5.7269e-02,\n",
       "         2.4679e-02, -3.6832e-02,  8.0152e-03,  2.8096e-02, -2.7611e-02,\n",
       "        -2.1234e-02,  9.4131e-03, -3.2056e-02, -1.6453e-02, -2.9303e-02,\n",
       "        -8.4207e-04,  3.3879e-02,  1.9581e-02, -1.8061e-02,  2.5626e-02,\n",
       "        -5.2765e-03,  9.6224e-03,  2.1106e-02, -2.1586e-03,  2.6872e-02,\n",
       "        -4.3786e-02, -8.9391e-03,  1.0714e-01, -1.1791e-02,  3.9609e-02,\n",
       "         1.0795e-02,  1.4064e-02, -3.2212e-02,  1.3570e-02, -7.5017e-03,\n",
       "        -1.8057e-02,  2.7827e-02,  6.5414e-02,  9.1749e-03,  2.4934e-02,\n",
       "         8.8799e-02, -5.9105e-02,  2.6519e-02, -5.1862e-02,  1.1190e-02,\n",
       "         4.2712e-02,  9.8429e-03, -1.2716e-02,  3.0474e-02,  2.2211e-02,\n",
       "        -2.7056e-03,  3.4255e-03, -2.2478e-02, -2.9303e-02, -5.3857e-02,\n",
       "        -2.5008e-02, -1.6887e-02,  5.3174e-02,  5.8757e-02, -2.4411e-02,\n",
       "        -2.8033e-02,  1.5272e-02,  7.0413e-02, -6.2312e-02, -5.3609e-02,\n",
       "         2.8049e-02, -2.8251e-03, -4.0838e-02,  5.8643e-02, -1.8121e-02,\n",
       "        -3.1843e-02,  3.3168e-02, -1.4853e-02,  3.1639e-02,  1.8819e-02,\n",
       "         4.1410e-02,  3.2840e-02, -2.8920e-02,  4.1682e-02, -2.1307e-02,\n",
       "        -2.9484e-02,  2.0970e-03, -4.6762e-02,  3.2188e-02,  4.0798e-02,\n",
       "         1.7910e-02,  7.8617e-02, -1.1936e-02,  6.1108e-03, -4.0116e-02,\n",
       "        -4.1864e-02, -2.9407e-03, -6.3874e-02, -9.1805e-03,  2.9604e-02,\n",
       "        -4.2288e-03,  5.9424e-03, -1.9690e-02, -2.5340e-02,  6.3299e-02,\n",
       "        -3.3443e-02, -7.3142e-02,  5.9521e-02,  4.4950e-02, -1.4194e-02,\n",
       "         3.1887e-02, -1.0793e-02,  3.7461e-02,  6.2864e-02,  3.1783e-02,\n",
       "         3.6497e-02, -8.0708e-02,  2.5995e-02, -1.5623e-02,  6.5559e-04,\n",
       "         1.0634e-02,  7.3257e-02,  9.5946e-03, -2.3390e-02,  2.6338e-02,\n",
       "         1.2060e-03,  2.2389e-02,  1.3712e-03,  4.4090e-02, -7.9224e-04,\n",
       "         4.2235e-02,  2.5867e-02,  1.3162e-03, -2.2668e-03, -1.2221e-02,\n",
       "        -9.8808e-03,  6.5437e-02,  6.5594e-03,  2.7332e-02, -2.7124e-02,\n",
       "        -2.1327e-02, -1.1086e-01,  3.4498e-02, -9.0437e-02,  4.3541e-02,\n",
       "         4.7504e-02, -4.8786e-03,  1.0602e-02, -1.2713e-02, -4.9040e-02,\n",
       "        -3.0974e-02,  1.3892e-02, -5.9169e-02, -4.2717e-04, -9.6568e-03,\n",
       "        -1.4802e-02,  6.6586e-02,  1.9838e-02,  3.5917e-02, -4.7298e-02,\n",
       "        -2.5784e-02, -4.3674e-02, -8.3752e-03,  5.7444e-02, -2.9454e-02,\n",
       "         1.6027e-02,  8.4918e-02, -2.0287e-02, -6.7611e-03, -9.1056e-03,\n",
       "         1.5142e-03, -2.0916e-02, -1.4511e-02,  3.1490e-02, -3.7785e-02,\n",
       "         7.6432e-02, -1.6680e-03, -3.1206e-02, -1.2895e-02, -2.3242e-02,\n",
       "        -6.3763e-03, -8.1756e-03, -6.3529e-03,  1.1537e-01, -2.8023e-02,\n",
       "        -4.6115e-03, -4.5971e-02, -1.8949e-02, -2.9260e-02, -1.5813e-02,\n",
       "         6.9489e-03,  1.8818e-02, -2.0096e-02, -2.9280e-02, -1.1657e-02,\n",
       "         1.1393e-02,  5.5468e-02,  2.1043e-03, -7.8144e-02, -4.8677e-02,\n",
       "         5.2707e-03, -4.2636e-02, -7.9543e-03,  5.8197e-02,  3.9361e-02,\n",
       "         3.2325e-02, -9.3471e-02, -7.6141e-03,  4.1071e-02, -6.1979e-02,\n",
       "        -3.3320e-02,  2.1425e-02, -7.2907e-03,  2.1977e-02,  3.1507e-02,\n",
       "         2.3402e-02,  3.6607e-02,  1.7172e-02, -2.5609e-02,  8.3261e-03,\n",
       "        -4.8489e-02,  2.8240e-02,  4.9536e-02, -3.9929e-02,  4.4933e-02,\n",
       "         3.8055e-02,  3.1321e-02,  7.9804e-04,  2.6024e-04,  1.9867e-02,\n",
       "        -5.0716e-02, -4.9561e-02, -1.1833e-02,  1.2025e-03, -9.2038e-03,\n",
       "        -1.2499e-02, -7.8082e-05, -8.1305e-03, -1.4263e-02, -2.5248e-02,\n",
       "         7.2655e-02,  1.8218e-02,  5.7825e-04,  3.9609e-02, -2.1993e-02,\n",
       "        -1.4411e-02, -2.0578e-02, -6.3503e-02,  8.7816e-02,  9.9848e-02,\n",
       "         3.6116e-02,  1.4350e-02, -1.0299e-02, -1.2724e-02,  3.7241e-02,\n",
       "        -2.2885e-02,  9.0006e-03,  3.3443e-03, -1.0846e-03,  7.3895e-02,\n",
       "         6.6728e-03,  1.0247e-01, -3.3557e-03, -8.0838e-03,  6.5000e-02,\n",
       "         2.4842e-02,  5.4939e-02,  4.6679e-02,  2.1724e-02, -2.6937e-03,\n",
       "        -1.1027e-02, -1.1108e-02, -3.0016e-02, -7.8484e-04, -1.3244e-02,\n",
       "         2.0725e-02,  1.6452e-02, -1.5243e-02,  3.6642e-02, -2.2963e-03,\n",
       "        -1.8034e-02, -2.5066e-02,  4.6599e-02, -8.5512e-05,  8.9678e-03,\n",
       "        -3.3664e-02,  7.0156e-02, -2.0272e-02,  4.5799e-03, -2.6626e-02,\n",
       "         3.3543e-02,  2.6905e-02,  1.2595e-02, -2.0541e-02,  7.2589e-03,\n",
       "         1.9421e-02,  1.3791e-02,  1.5251e-02, -5.7903e-02,  2.4950e-03,\n",
       "        -2.5049e-02,  7.2171e-02,  1.7014e-02, -5.9599e-03,  1.6013e-02,\n",
       "        -2.0308e-02,  6.9116e-02,  1.7203e-02,  1.8026e-02, -8.7789e-03,\n",
       "         6.3039e-02, -3.2999e-02,  4.9009e-02, -2.4279e-02, -2.8046e-02,\n",
       "        -9.0558e-02, -6.0332e-03, -2.4357e-02, -5.2651e-02,  2.1197e-02,\n",
       "         2.7354e-02, -1.8809e-02, -2.1353e-02,  1.3443e-02, -2.0206e-02,\n",
       "         4.0903e-02,  6.3608e-02,  4.5427e-02,  3.1540e-02,  6.0856e-02,\n",
       "        -6.6828e-02,  3.9194e-02, -1.7301e-03, -9.2966e-03,  4.5338e-02,\n",
       "        -4.6712e-02,  1.7642e-02,  1.4884e-02,  4.6037e-02,  1.0257e-02,\n",
       "        -2.2854e-02, -8.6698e-03,  5.8998e-02, -3.0392e-03,  4.0358e-02,\n",
       "         3.7453e-02,  8.9137e-03, -7.8894e-03,  2.4396e-02,  2.5549e-02,\n",
       "         7.5864e-03,  5.3277e-03, -3.4836e-04, -2.1883e-02, -4.2866e-02,\n",
       "         1.4998e-02,  1.1550e-01, -1.0443e-02, -7.0825e-03, -1.2403e-02,\n",
       "        -8.4626e-03,  5.9922e-03, -3.7924e-02, -3.8725e-02,  6.9425e-02,\n",
       "         1.3768e-02,  2.4670e-02,  1.2125e-03, -2.2723e-02, -4.4029e-02,\n",
       "        -2.1189e-02,  1.4532e-02, -2.8225e-02,  2.3992e-02,  2.1545e-02,\n",
       "        -1.2228e-01,  4.6525e-03,  5.9291e-03, -6.8360e-02,  6.7459e-03,\n",
       "         3.6912e-03, -4.2070e-02, -4.6315e-02, -2.9123e-02,  5.3284e-02,\n",
       "        -2.7712e-02, -5.5762e-02,  3.6863e-02,  3.0631e-02,  4.4769e-02,\n",
       "         1.2678e-02,  6.9450e-02,  1.0504e-02, -6.5864e-03, -2.1399e-03,\n",
       "        -4.7422e-02, -5.2341e-02, -4.1884e-02, -4.9888e-02, -8.3685e-05,\n",
       "        -5.6606e-02,  2.1377e-02, -1.7146e-02,  2.3362e-02, -1.8858e-02,\n",
       "         3.2443e-02, -4.3904e-02, -5.8630e-02, -7.8096e-02, -1.0861e-02,\n",
       "        -1.0419e-02, -4.4453e-02,  1.0825e-02,  2.8392e-02, -5.0853e-02,\n",
       "         1.8749e-02, -3.0194e-02,  1.9081e-02,  5.1419e-02, -3.1336e-02,\n",
       "         5.3451e-03,  3.3034e-02,  2.1458e-02, -1.3298e-02, -7.4668e-02,\n",
       "        -1.1275e-02,  1.7705e-02,  1.1272e-02,  7.5984e-03, -3.0295e-02,\n",
       "        -4.3982e-03,  1.6319e-02,  1.4076e-02, -4.1376e-02,  1.0128e-02,\n",
       "        -2.0368e-02, -3.1456e-02, -3.7889e-02, -3.0890e-02, -2.2307e-02,\n",
       "        -6.5941e-02,  5.9740e-02,  1.5615e-03,  3.1790e-02, -3.8396e-02,\n",
       "        -3.9803e-02,  1.4816e-02,  3.3378e-02,  6.4884e-02, -1.5380e-02,\n",
       "        -4.2848e-02, -5.5220e-02, -1.4752e-02, -2.2781e-02, -4.0729e-02,\n",
       "        -9.7884e-02, -3.4006e-02, -6.0603e-03, -1.7736e-02, -4.9818e-02,\n",
       "        -2.0603e-02,  3.4014e-03, -2.7338e-03, -4.4098e-03, -2.7845e-02,\n",
       "        -9.7056e-03,  4.5851e-02, -4.8568e-02, -7.4218e-02, -2.7894e-02,\n",
       "        -8.2891e-02, -5.1241e-02, -3.1124e-02, -3.6186e-02,  1.7490e-02,\n",
       "         8.0321e-02, -1.1280e-02,  6.8861e-02, -2.7668e-02, -7.2714e-02,\n",
       "        -2.5547e-02,  1.6521e-02,  4.7275e-02, -5.0370e-03,  2.7394e-02,\n",
       "        -7.8439e-03,  4.0686e-02,  2.9669e-02, -1.8812e-02, -4.8316e-02,\n",
       "         1.5971e-03, -1.2833e-03,  2.4820e-03,  7.2924e-03, -3.1136e-02,\n",
       "         3.2225e-02,  3.6828e-02, -1.1541e-02,  1.3722e-02,  5.8795e-02,\n",
       "         1.4651e-01,  3.5046e-02, -7.5133e-04,  1.8700e-03, -7.7914e-03,\n",
       "        -1.4466e-02, -4.1868e-02,  8.7149e-04, -4.7998e-02,  1.3397e-01,\n",
       "         5.2992e-02, -6.4812e-02, -2.2050e-02, -8.4143e-03, -1.5841e-02,\n",
       "        -4.0774e-03, -3.1162e-02, -1.7233e-02, -4.0946e-03, -4.1031e-02,\n",
       "         6.2564e-03, -6.5229e-02, -7.7184e-02, -1.0118e-02,  5.7305e-03,\n",
       "         3.2124e-02,  3.3173e-03, -9.0049e-03,  4.9780e-03, -3.4604e-02,\n",
       "        -6.0146e-02,  2.8397e-03,  3.9208e-02,  4.6980e-02, -2.1081e-02,\n",
       "         1.6185e-02, -3.0247e-02,  3.8992e-02, -4.4014e-02, -3.3474e-02,\n",
       "        -2.9279e-02,  4.1222e-02, -1.4694e-02,  5.9593e-02, -3.9422e-02,\n",
       "         1.4111e-02, -8.4495e-03, -4.3955e-02, -3.4950e-02, -1.3638e-02,\n",
       "         4.1646e-02,  5.1128e-03,  1.7467e-02,  2.4781e-02,  9.6630e-03,\n",
       "         6.1020e-02,  4.6698e-02,  4.9586e-02,  2.1348e-02, -2.5977e-02,\n",
       "         3.6753e-02, -2.1679e-02, -6.6642e-03, -1.1976e-02,  2.4779e-02,\n",
       "         3.8251e-02, -1.4935e-02, -1.5497e-02,  6.0689e-03, -4.2981e-02,\n",
       "        -7.4142e-03, -1.0696e-02,  6.3541e-02,  1.4579e-02, -1.7123e-02,\n",
       "        -1.9450e-02,  6.0776e-02, -4.1789e-02, -1.0715e-02,  1.2658e-02,\n",
       "         2.0553e-02,  4.5176e-02,  5.8150e-02,  3.1660e-02,  2.5916e-03,\n",
       "         3.1341e-02, -1.9876e-02,  4.1345e-02, -2.4788e-02, -7.5233e-03,\n",
       "        -3.1442e-02,  2.3873e-02,  6.7733e-04, -6.0955e-02, -1.0210e-02,\n",
       "         2.1181e-02, -1.9299e-02, -1.6425e-02])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['bert.encoder.layer.0.attention.self.query.weight'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.0461e-02,  6.9734e-02, -3.1827e-02,  2.9276e-03, -1.6021e-02,\n",
       "        -2.2153e-04,  2.4495e-02,  1.5918e-02, -1.5852e-02,  1.6580e-02,\n",
       "        -6.6527e-03, -6.8142e-02, -1.0439e-02, -2.2604e-02,  3.4094e-03,\n",
       "         5.0117e-02,  3.8181e-02,  1.5476e-02,  1.8190e-02, -1.7143e-02,\n",
       "         4.2855e-02, -1.1223e-02,  1.8801e-02,  3.8932e-02, -1.2070e-03,\n",
       "        -2.5220e-03,  1.1252e-02,  3.5875e-02, -1.1655e-02, -4.4611e-02,\n",
       "        -3.5723e-02,  2.7098e-02, -2.5315e-02, -4.3921e-02, -3.3590e-02,\n",
       "        -2.5316e-02, -2.5402e-03, -3.2093e-02,  1.7590e-02,  4.4715e-03,\n",
       "        -4.2622e-02, -5.4426e-02,  5.5715e-03,  3.6474e-02,  2.0075e-02,\n",
       "        -2.4211e-02,  1.1200e-01, -1.2124e-02,  6.2336e-03,  1.2563e-02,\n",
       "        -6.5182e-02, -5.8309e-03, -4.3623e-03, -6.7238e-02,  1.9629e-03,\n",
       "        -2.7483e-03,  6.2588e-03, -1.4956e-02, -3.1359e-02, -2.5026e-02,\n",
       "         3.3577e-02, -2.6373e-03, -5.5331e-03,  5.0805e-03, -8.1706e-04,\n",
       "         3.4413e-02, -3.2688e-02,  4.0914e-02, -6.5257e-02, -6.4631e-03,\n",
       "        -9.8864e-03, -4.4526e-02, -2.3206e-02, -3.2798e-02, -3.1135e-02,\n",
       "        -2.6865e-02, -2.0433e-02,  2.9545e-03,  2.2388e-02, -6.1680e-02,\n",
       "        -3.3148e-02, -5.2619e-02, -2.7528e-02,  2.7317e-03, -2.7820e-02,\n",
       "        -6.3700e-02, -1.9414e-02,  8.1146e-03, -5.4659e-03, -2.3526e-02,\n",
       "         1.3108e-02, -1.0642e-02, -4.9542e-02, -4.3532e-02, -8.0338e-03,\n",
       "         6.3971e-03, -4.0212e-02,  1.5706e-03, -1.0178e-01,  2.0558e-02,\n",
       "        -2.9127e-03, -2.5191e-02, -1.8479e-02, -1.2180e-02, -7.4657e-02,\n",
       "        -1.6298e-02, -3.9536e-02, -3.0868e-03, -2.2938e-02,  1.2452e-02,\n",
       "        -2.4680e-02, -5.1852e-02,  1.1365e-02,  1.4444e-02, -4.0203e-02,\n",
       "        -2.3351e-02, -8.1960e-02, -3.5327e-02, -7.2629e-03,  4.5034e-02,\n",
       "        -2.1541e-02, -2.2827e-02, -7.1737e-02,  9.3647e-02, -2.1151e-02,\n",
       "        -7.9203e-03, -6.5834e-02, -1.0937e-02, -3.8643e-02, -5.5013e-03,\n",
       "         1.3098e-02,  2.6717e-02, -5.9272e-03, -4.6606e-02,  5.2297e-02,\n",
       "        -4.7664e-02, -3.5202e-02,  6.7434e-02, -3.8803e-02, -2.7904e-02,\n",
       "        -3.4478e-02,  1.1120e-02,  8.7640e-02, -9.5207e-02, -2.1374e-03,\n",
       "        -1.2991e-02, -3.6576e-02, -3.6756e-02, -2.1469e-02, -3.6786e-03,\n",
       "        -2.0379e-02, -2.3858e-02,  4.1728e-03, -5.9203e-02, -3.9413e-02,\n",
       "        -8.6721e-02,  1.5371e-02, -2.9499e-02, -9.3820e-02, -2.3789e-02,\n",
       "        -5.8569e-03, -6.7406e-03,  4.7113e-02, -1.0630e-02, -7.6386e-02,\n",
       "        -1.7571e-02, -9.6614e-02, -7.7073e-02,  6.3863e-03,  3.2088e-02,\n",
       "        -2.9534e-02,  2.5691e-02, -8.3912e-02,  1.0607e-02, -6.4535e-02,\n",
       "         2.3283e-02,  1.6866e-02, -5.7672e-02, -1.9704e-02, -4.8670e-02,\n",
       "        -4.3245e-01,  2.8337e-02, -5.9752e-03, -1.6309e-02, -1.9667e-02,\n",
       "         1.4552e-02, -2.9826e-02, -3.6280e-02, -5.5541e-03,  1.1208e-02,\n",
       "        -8.7074e-02,  2.2057e-02,  3.3955e-02, -3.7634e-02,  2.7877e-02,\n",
       "        -3.0927e-02, -1.8918e-02, -8.6372e-03, -4.6113e-02, -5.9039e-03,\n",
       "        -7.9173e-03, -7.4250e-02, -3.4472e-02, -1.9592e-02, -3.9857e-02,\n",
       "        -1.0622e-01, -3.9181e-02, -5.6377e-02, -2.3781e-02, -4.8137e-02,\n",
       "        -1.6482e-02, -2.9792e-02,  1.3911e-02,  2.2846e-02,  2.6553e-03,\n",
       "         1.8781e-03, -1.5505e-02,  1.3099e-02, -4.2984e-02,  3.7213e-02,\n",
       "         2.2090e-02, -2.8861e-02, -5.1158e-02,  1.8219e-02, -1.7329e-02,\n",
       "         1.1503e-02, -4.5019e-02, -2.3348e-02,  8.0448e-03,  8.4590e-02,\n",
       "        -1.5335e-02, -1.2483e-03,  5.5378e-02, -4.8111e-02,  2.0164e-02,\n",
       "         2.6325e-03, -6.6506e-02, -3.3015e-02, -6.3276e-02,  1.7359e-03,\n",
       "         1.9304e-02,  2.6042e-02,  1.2434e-03, -4.7096e-03, -6.5213e-02,\n",
       "        -3.4601e-02, -3.1804e-02,  2.9807e-02,  2.7922e-02, -5.9616e-03,\n",
       "        -1.1679e-02,  5.4256e-02,  2.5791e-02, -3.0218e-04,  4.8376e-02,\n",
       "        -2.4557e-02, -1.8509e-04, -6.0040e-02, -1.4468e-02,  8.4066e-02,\n",
       "        -1.9645e-02, -2.2442e-02,  3.6608e-02, -3.9204e-02, -4.5903e-02,\n",
       "        -7.9731e-02, -1.6310e-02, -1.7317e-02,  2.6027e-02, -2.5780e-02,\n",
       "         9.8776e-03,  1.0848e-02,  2.3895e-02,  2.8743e-02,  2.2569e-03,\n",
       "         2.0751e-02,  1.0606e-02, -4.7199e-02, -4.1201e-02, -1.3567e-04,\n",
       "         9.0842e-03, -2.0445e-02, -3.4840e-02, -4.2235e-02, -5.3990e-02,\n",
       "         5.9834e-03,  2.1033e-02,  3.8490e-02, -1.9910e-02, -2.5285e-02,\n",
       "         9.3380e-02, -5.1092e-03, -7.6414e-02, -1.6556e-02,  1.1960e-02,\n",
       "         2.5851e-02,  1.4499e-02, -3.5933e-03, -4.7073e-02,  6.2363e-02,\n",
       "         2.3443e-02,  6.2648e-03, -2.2403e-03,  5.2532e-03,  3.3447e-02,\n",
       "        -6.4382e-02, -1.3196e-02,  1.5456e-02, -6.0327e-02, -3.6905e-03,\n",
       "        -6.5401e-02,  5.6967e-03, -1.4881e-02,  2.5567e-02,  2.4739e-02,\n",
       "         4.7662e-02, -5.6442e-02,  2.8840e-02, -8.3698e-02,  1.0169e-03,\n",
       "         4.7555e-03,  4.4931e-02, -3.2421e-02, -7.0913e-02,  1.6236e-02,\n",
       "        -3.6104e-02, -6.9623e-02, -3.6530e-02, -6.9892e-02, -5.6053e-03,\n",
       "         2.2984e-03,  8.6244e-03,  1.4104e-02, -1.5690e-02,  2.6717e-02,\n",
       "        -5.5821e-02,  2.7170e-02, -2.2368e-02, -8.3826e-02, -8.7613e-04,\n",
       "         1.9288e-02,  2.8810e-03, -1.4074e-02,  2.0648e-02, -3.4879e-02,\n",
       "        -4.7858e-02,  9.1133e-03, -2.1065e-02, -2.5766e-02, -9.2934e-02,\n",
       "        -1.7523e-02,  1.3592e-02,  7.5672e-02, -3.6955e-02, -4.6586e-02,\n",
       "        -2.1423e-02, -4.9157e-02,  3.9123e-04,  3.1437e-03, -6.8067e-02,\n",
       "        -6.4921e-02, -1.7594e-02, -5.3668e-02,  3.0253e-02,  4.9147e-02,\n",
       "         2.8400e-03, -8.0255e-02, -2.9412e-02, -1.9213e-02, -7.2743e-02,\n",
       "        -3.8691e-02, -8.4402e-02, -2.8985e-03, -3.5233e-02, -1.6879e-02,\n",
       "         2.0052e-02, -2.5799e-02, -4.7550e-02, -3.6995e-02, -5.6413e-02,\n",
       "        -2.8841e-02,  6.2059e-02, -4.5011e-04,  4.5749e-03,  1.4445e-02,\n",
       "         3.2280e-03, -6.5737e-02, -5.2960e-02,  3.8755e-02, -3.5603e-02,\n",
       "        -7.3092e-02, -1.5250e-02,  2.4744e-02, -5.3124e-02, -3.3436e-02,\n",
       "        -6.3636e-02, -2.0714e-02, -4.7448e-02,  2.6807e-02,  3.6495e-02,\n",
       "        -9.0488e-02, -2.0358e-02, -2.6481e-02,  1.6073e-02, -3.7152e-02,\n",
       "         4.3042e-04, -2.8287e-02, -9.5890e-03, -1.1384e-02, -3.1517e-02,\n",
       "         7.5152e-03,  1.5033e-02, -2.7644e-03, -2.4451e-02, -4.6408e-03,\n",
       "        -7.7756e-02, -3.6060e-02, -5.3312e-02, -6.5552e-03, -1.0061e-01,\n",
       "        -5.9083e-02, -1.2747e-02, -5.8002e-02, -5.8252e-02, -8.1660e-02,\n",
       "        -4.1916e-02,  3.3674e-02,  4.0159e-02, -8.3144e-03, -5.1558e-03,\n",
       "        -2.8475e-02, -1.8313e-02,  1.7841e-03, -2.7036e-02,  1.6391e-02,\n",
       "         5.0225e-02, -7.9997e-03, -5.0349e-02, -3.0719e-02, -2.8085e-02,\n",
       "         1.2934e-03, -7.9958e-02, -3.7033e-02, -5.4457e-02, -3.4235e-02,\n",
       "        -2.4562e-02, -9.4467e-02, -3.1594e-02, -1.9532e-02, -7.8766e-02,\n",
       "        -5.5911e-02, -5.1251e-02,  6.8287e-03,  2.3901e-03,  1.7341e-02,\n",
       "        -1.8982e-02, -2.9259e-02,  3.0352e-02, -3.9701e-04,  3.0513e-03,\n",
       "        -2.7301e-02, -2.9516e-02, -5.3280e-02, -3.6616e-02, -8.8687e-03,\n",
       "         4.0112e-02,  1.5480e-02, -5.7062e-02,  4.7703e-02, -2.7718e-02,\n",
       "         9.1597e-02, -3.3947e-02,  1.3608e-02, -2.8517e-02, -4.5752e-02,\n",
       "        -6.7585e-03, -4.2150e-02, -5.9839e-02, -4.6429e-02, -1.5811e-02,\n",
       "        -5.3163e-02, -2.8281e-02,  4.4063e-02,  8.1345e-03,  5.0199e-02,\n",
       "        -5.3898e-02, -3.7279e-02, -5.6216e-02, -2.1727e-02,  3.9999e-03,\n",
       "        -6.4339e-03, -1.7183e-02, -4.4276e-02, -1.2356e-02,  1.2241e-02,\n",
       "        -1.3105e-02, -1.0988e-02, -2.2333e-02,  4.3900e-03,  6.5215e-02,\n",
       "         1.5148e-02, -3.6541e-02, -2.8720e-02,  1.6120e-05, -8.2765e-02,\n",
       "         9.5096e-03, -2.2613e-02, -8.7813e-02, -5.6625e-02, -1.4889e-02,\n",
       "         8.6411e-03, -2.5157e-02,  8.4695e-03, -1.0625e-02, -3.1585e-02,\n",
       "         7.2671e-02, -4.4576e-02,  6.5375e-02, -5.8678e-02, -6.8635e-02,\n",
       "        -4.9255e-03,  3.0885e-02, -4.0078e-02, -1.1824e-02, -3.9481e-02,\n",
       "        -3.7534e-02, -2.8148e-02, -6.7783e-02, -4.2159e-02,  7.4410e-04,\n",
       "        -3.1102e-02,  9.7771e-03, -6.2143e-02, -1.6781e-02,  4.5281e-02,\n",
       "         2.0445e-02,  1.1449e-02,  9.8312e-05, -5.3495e-02, -5.9770e-02,\n",
       "        -1.2183e-02, -1.5877e-02,  4.3193e-02, -8.7390e-02,  3.0446e-02,\n",
       "        -4.0469e-03,  6.6696e-03,  2.5148e-02, -9.2288e-03,  4.7478e-02,\n",
       "         1.4913e-03, -1.8715e-02,  1.2482e-02,  4.2584e-03, -4.4926e-02,\n",
       "         2.9420e-02,  3.6180e-02, -1.0555e-02, -2.7127e-02,  1.3765e-02,\n",
       "         6.8694e-03, -6.9724e-02,  1.4761e-02, -3.2281e-02, -6.4907e-02,\n",
       "         1.2463e-02, -1.8050e-02, -6.7536e-03, -2.2448e-02, -2.7862e-02,\n",
       "        -2.7647e-02,  7.2892e-02,  1.8628e-03, -4.6115e-02, -5.6270e-02,\n",
       "        -3.1500e-02, -4.7854e-02, -4.5692e-02, -2.0432e-02, -4.4274e-02,\n",
       "        -8.8417e-03, -2.1905e-02,  3.6729e-02,  3.7687e-02, -2.4796e-03,\n",
       "         1.0819e-03, -3.2691e-02, -9.0253e-02, -3.0312e-03,  4.6084e-02,\n",
       "         2.0369e-02, -7.0231e-02, -2.7333e-02, -8.7663e-02,  8.5675e-03,\n",
       "        -2.2886e-03, -4.8087e-02,  2.1552e-02,  1.4463e-02, -1.6634e-03,\n",
       "        -3.7142e-03,  2.1196e-02,  1.4076e-02,  1.5722e-02,  2.6415e-02,\n",
       "         2.6921e-02, -1.9087e-02, -1.5705e-02, -2.0968e-03,  4.8123e-02,\n",
       "        -5.7070e-02, -2.8769e-02, -4.2321e-03,  9.9865e-03, -1.7661e-02,\n",
       "        -3.1574e-02,  7.2262e-03,  3.2572e-02, -1.0170e-01, -2.3048e-02,\n",
       "         2.4829e-02,  2.5639e-02, -8.7369e-03, -6.6024e-03, -3.4674e-02,\n",
       "        -1.1234e-02,  4.3441e-02,  1.6480e-02, -1.9962e-02, -5.5474e-02,\n",
       "         3.4495e-02,  1.7296e-02,  1.4537e-02,  1.4691e-02, -3.5560e-02,\n",
       "         1.0574e-02,  2.0935e-02, -1.1333e-02, -4.3980e-02, -1.5276e-02,\n",
       "        -1.7890e-02,  5.8147e-03,  3.8492e-02, -2.8828e-02,  4.0850e-02,\n",
       "        -5.2343e-02, -4.2216e-02,  3.3646e-02,  5.4906e-03,  1.7957e-02,\n",
       "        -1.8939e-02,  4.5502e-02, -1.5830e-02,  7.4741e-02, -1.3465e-02,\n",
       "        -1.9749e-02,  4.5204e-02,  5.2851e-02, -3.6431e-02,  2.3607e-02,\n",
       "        -2.8735e-02, -4.4660e-02,  7.7867e-02, -5.3180e-02, -4.4169e-02,\n",
       "        -1.9887e-02, -1.3428e-02,  5.7163e-02, -3.0353e-02, -8.1256e-03,\n",
       "         1.9528e-02, -9.0187e-03, -6.1055e-02, -4.0301e-02,  2.1307e-02,\n",
       "        -2.1109e-03, -5.1682e-02,  3.5288e-02, -1.1608e-02, -1.1204e-02,\n",
       "         1.0502e-02, -4.7630e-02,  2.7808e-03,  5.8654e-02,  7.8917e-03,\n",
       "         2.4163e-02,  1.7546e-02,  6.3508e-03, -4.6726e-02,  1.7979e-02,\n",
       "         8.0103e-03, -5.7780e-02,  2.5366e-02,  2.2173e-02,  3.1421e-02,\n",
       "         1.1779e-02,  6.2488e-03, -1.0969e-02,  1.2830e-02, -3.2279e-02,\n",
       "        -4.2476e-02, -5.0352e-02, -6.9797e-03,  6.5312e-02, -5.9164e-02,\n",
       "         4.3400e-02,  5.4692e-03, -1.8442e-02,  8.8275e-03, -9.5528e-03,\n",
       "        -4.3329e-02, -6.2202e-02, -2.0282e-02, -5.0583e-02, -7.7592e-03,\n",
       "        -3.5848e-02, -1.4712e-02,  5.9468e-02, -6.7132e-02, -4.5774e-02,\n",
       "        -1.6080e-01, -2.0764e-03, -1.1149e-01,  2.5021e-03, -2.6869e-02,\n",
       "        -1.1405e-02, -3.7546e-02, -3.8675e-02,  1.7745e-02,  5.7616e-03,\n",
       "        -5.5724e-02, -6.6041e-02, -9.0923e-03, -9.3419e-02, -9.3210e-03,\n",
       "        -7.2311e-02, -9.5221e-02,  6.0981e-02, -1.3484e-02, -3.4046e-02,\n",
       "        -3.1859e-03,  6.5745e-03, -5.2395e-02, -4.4301e-02, -5.4276e-02,\n",
       "         3.6095e-02, -4.1105e-02,  2.1378e-02,  2.4527e-02, -2.3856e-02,\n",
       "         2.1367e-02, -9.4081e-03,  5.9979e-02,  1.2796e-02, -1.1280e-01,\n",
       "        -5.2191e-02,  1.0212e-03,  6.6099e-03, -5.4558e-03, -1.5545e-02,\n",
       "        -2.0038e-02,  2.7379e-02, -6.4059e-02,  2.5964e-02, -3.4578e-02,\n",
       "         1.9719e-02, -2.0732e-02, -1.5417e-02])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_generated_dict['bert.encoder.layer.11.attention.self.query.weight'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.0461e-02,  6.9734e-02, -3.1827e-02,  2.9276e-03, -1.6021e-02,\n",
       "        -2.2153e-04,  2.4495e-02,  1.5918e-02, -1.5852e-02,  1.6580e-02,\n",
       "        -6.6527e-03, -6.8142e-02, -1.0439e-02, -2.2604e-02,  3.4094e-03,\n",
       "         5.0117e-02,  3.8181e-02,  1.5476e-02,  1.8190e-02, -1.7143e-02,\n",
       "         4.2855e-02, -1.1223e-02,  1.8801e-02,  3.8932e-02, -1.2070e-03,\n",
       "        -2.5220e-03,  1.1252e-02,  3.5875e-02, -1.1655e-02, -4.4611e-02,\n",
       "        -3.5723e-02,  2.7098e-02, -2.5315e-02, -4.3921e-02, -3.3590e-02,\n",
       "        -2.5316e-02, -2.5402e-03, -3.2093e-02,  1.7590e-02,  4.4715e-03,\n",
       "        -4.2622e-02, -5.4426e-02,  5.5715e-03,  3.6474e-02,  2.0075e-02,\n",
       "        -2.4211e-02,  1.1200e-01, -1.2124e-02,  6.2336e-03,  1.2563e-02,\n",
       "        -6.5182e-02, -5.8309e-03, -4.3623e-03, -6.7238e-02,  1.9629e-03,\n",
       "        -2.7483e-03,  6.2588e-03, -1.4956e-02, -3.1359e-02, -2.5026e-02,\n",
       "         3.3577e-02, -2.6373e-03, -5.5331e-03,  5.0805e-03, -8.1706e-04,\n",
       "         3.4413e-02, -3.2688e-02,  4.0914e-02, -6.5257e-02, -6.4631e-03,\n",
       "        -9.8864e-03, -4.4526e-02, -2.3206e-02, -3.2798e-02, -3.1135e-02,\n",
       "        -2.6865e-02, -2.0433e-02,  2.9545e-03,  2.2388e-02, -6.1680e-02,\n",
       "        -3.3148e-02, -5.2619e-02, -2.7528e-02,  2.7317e-03, -2.7820e-02,\n",
       "        -6.3700e-02, -1.9414e-02,  8.1146e-03, -5.4659e-03, -2.3526e-02,\n",
       "         1.3108e-02, -1.0642e-02, -4.9542e-02, -4.3532e-02, -8.0338e-03,\n",
       "         6.3971e-03, -4.0212e-02,  1.5706e-03, -1.0178e-01,  2.0558e-02,\n",
       "        -2.9127e-03, -2.5191e-02, -1.8479e-02, -1.2180e-02, -7.4657e-02,\n",
       "        -1.6298e-02, -3.9536e-02, -3.0868e-03, -2.2938e-02,  1.2452e-02,\n",
       "        -2.4680e-02, -5.1852e-02,  1.1365e-02,  1.4444e-02, -4.0203e-02,\n",
       "        -2.3351e-02, -8.1960e-02, -3.5327e-02, -7.2629e-03,  4.5034e-02,\n",
       "        -2.1541e-02, -2.2827e-02, -7.1737e-02,  9.3647e-02, -2.1151e-02,\n",
       "        -7.9203e-03, -6.5834e-02, -1.0937e-02, -3.8643e-02, -5.5013e-03,\n",
       "         1.3098e-02,  2.6717e-02, -5.9272e-03, -4.6606e-02,  5.2297e-02,\n",
       "        -4.7664e-02, -3.5202e-02,  6.7434e-02, -3.8803e-02, -2.7904e-02,\n",
       "        -3.4478e-02,  1.1120e-02,  8.7640e-02, -9.5207e-02, -2.1374e-03,\n",
       "        -1.2991e-02, -3.6576e-02, -3.6756e-02, -2.1469e-02, -3.6786e-03,\n",
       "        -2.0379e-02, -2.3858e-02,  4.1728e-03, -5.9203e-02, -3.9413e-02,\n",
       "        -8.6721e-02,  1.5371e-02, -2.9499e-02, -9.3820e-02, -2.3789e-02,\n",
       "        -5.8569e-03, -6.7406e-03,  4.7113e-02, -1.0630e-02, -7.6386e-02,\n",
       "        -1.7571e-02, -9.6614e-02, -7.7073e-02,  6.3863e-03,  3.2088e-02,\n",
       "        -2.9534e-02,  2.5691e-02, -8.3912e-02,  1.0607e-02, -6.4535e-02,\n",
       "         2.3283e-02,  1.6866e-02, -5.7672e-02, -1.9704e-02, -4.8670e-02,\n",
       "        -4.3245e-01,  2.8337e-02, -5.9752e-03, -1.6309e-02, -1.9667e-02,\n",
       "         1.4552e-02, -2.9826e-02, -3.6280e-02, -5.5541e-03,  1.1208e-02,\n",
       "        -8.7074e-02,  2.2057e-02,  3.3955e-02, -3.7634e-02,  2.7877e-02,\n",
       "        -3.0927e-02, -1.8918e-02, -8.6372e-03, -4.6113e-02, -5.9039e-03,\n",
       "        -7.9173e-03, -7.4250e-02, -3.4472e-02, -1.9592e-02, -3.9857e-02,\n",
       "        -1.0622e-01, -3.9181e-02, -5.6377e-02, -2.3781e-02, -4.8137e-02,\n",
       "        -1.6482e-02, -2.9792e-02,  1.3911e-02,  2.2846e-02,  2.6553e-03,\n",
       "         1.8781e-03, -1.5505e-02,  1.3099e-02, -4.2984e-02,  3.7213e-02,\n",
       "         2.2090e-02, -2.8861e-02, -5.1158e-02,  1.8219e-02, -1.7329e-02,\n",
       "         1.1503e-02, -4.5019e-02, -2.3348e-02,  8.0448e-03,  8.4590e-02,\n",
       "        -1.5335e-02, -1.2483e-03,  5.5378e-02, -4.8111e-02,  2.0164e-02,\n",
       "         2.6325e-03, -6.6506e-02, -3.3015e-02, -6.3276e-02,  1.7359e-03,\n",
       "         1.9304e-02,  2.6042e-02,  1.2434e-03, -4.7096e-03, -6.5213e-02,\n",
       "        -3.4601e-02, -3.1804e-02,  2.9807e-02,  2.7922e-02, -5.9616e-03,\n",
       "        -1.1679e-02,  5.4256e-02,  2.5791e-02, -3.0218e-04,  4.8376e-02,\n",
       "        -2.4557e-02, -1.8509e-04, -6.0040e-02, -1.4468e-02,  8.4066e-02,\n",
       "        -1.9645e-02, -2.2442e-02,  3.6608e-02, -3.9204e-02, -4.5903e-02,\n",
       "        -7.9731e-02, -1.6310e-02, -1.7317e-02,  2.6027e-02, -2.5780e-02,\n",
       "         9.8776e-03,  1.0848e-02,  2.3895e-02,  2.8743e-02,  2.2569e-03,\n",
       "         2.0751e-02,  1.0606e-02, -4.7199e-02, -4.1201e-02, -1.3567e-04,\n",
       "         9.0842e-03, -2.0445e-02, -3.4840e-02, -4.2235e-02, -5.3990e-02,\n",
       "         5.9834e-03,  2.1033e-02,  3.8490e-02, -1.9910e-02, -2.5285e-02,\n",
       "         9.3380e-02, -5.1092e-03, -7.6414e-02, -1.6556e-02,  1.1960e-02,\n",
       "         2.5851e-02,  1.4499e-02, -3.5933e-03, -4.7073e-02,  6.2363e-02,\n",
       "         2.3443e-02,  6.2648e-03, -2.2403e-03,  5.2532e-03,  3.3447e-02,\n",
       "        -6.4382e-02, -1.3196e-02,  1.5456e-02, -6.0327e-02, -3.6905e-03,\n",
       "        -6.5401e-02,  5.6967e-03, -1.4881e-02,  2.5567e-02,  2.4739e-02,\n",
       "         4.7662e-02, -5.6442e-02,  2.8840e-02, -8.3698e-02,  1.0169e-03,\n",
       "         4.7555e-03,  4.4931e-02, -3.2421e-02, -7.0913e-02,  1.6236e-02,\n",
       "        -3.6104e-02, -6.9623e-02, -3.6530e-02, -6.9892e-02, -5.6053e-03,\n",
       "         2.2984e-03,  8.6244e-03,  1.4104e-02, -1.5690e-02,  2.6717e-02,\n",
       "        -5.5821e-02,  2.7170e-02, -2.2368e-02, -8.3826e-02, -8.7613e-04,\n",
       "         1.9288e-02,  2.8810e-03, -1.4074e-02,  2.0648e-02, -3.4879e-02,\n",
       "        -4.7858e-02,  9.1133e-03, -2.1065e-02, -2.5766e-02, -9.2934e-02,\n",
       "        -1.7523e-02,  1.3592e-02,  7.5672e-02, -3.6955e-02, -4.6586e-02,\n",
       "        -2.1423e-02, -4.9157e-02,  3.9123e-04,  3.1437e-03, -6.8067e-02,\n",
       "        -6.4921e-02, -1.7594e-02, -5.3668e-02,  3.0253e-02,  4.9147e-02,\n",
       "         2.8400e-03, -8.0255e-02, -2.9412e-02, -1.9213e-02, -7.2743e-02,\n",
       "        -3.8691e-02, -8.4402e-02, -2.8985e-03, -3.5233e-02, -1.6879e-02,\n",
       "         2.0052e-02, -2.5799e-02, -4.7550e-02, -3.6995e-02, -5.6413e-02,\n",
       "        -2.8841e-02,  6.2059e-02, -4.5011e-04,  4.5749e-03,  1.4445e-02,\n",
       "         3.2280e-03, -6.5737e-02, -5.2960e-02,  3.8755e-02, -3.5603e-02,\n",
       "        -7.3092e-02, -1.5250e-02,  2.4744e-02, -5.3124e-02, -3.3436e-02,\n",
       "        -6.3636e-02, -2.0714e-02, -4.7448e-02,  2.6807e-02,  3.6495e-02,\n",
       "        -9.0488e-02, -2.0358e-02, -2.6481e-02,  1.6073e-02, -3.7152e-02,\n",
       "         4.3042e-04, -2.8287e-02, -9.5890e-03, -1.1384e-02, -3.1517e-02,\n",
       "         7.5152e-03,  1.5033e-02, -2.7644e-03, -2.4451e-02, -4.6408e-03,\n",
       "        -7.7756e-02, -3.6060e-02, -5.3312e-02, -6.5552e-03, -1.0061e-01,\n",
       "        -5.9083e-02, -1.2747e-02, -5.8002e-02, -5.8252e-02, -8.1660e-02,\n",
       "        -4.1916e-02,  3.3674e-02,  4.0159e-02, -8.3144e-03, -5.1558e-03,\n",
       "        -2.8475e-02, -1.8313e-02,  1.7841e-03, -2.7036e-02,  1.6391e-02,\n",
       "         5.0225e-02, -7.9997e-03, -5.0349e-02, -3.0719e-02, -2.8085e-02,\n",
       "         1.2934e-03, -7.9958e-02, -3.7033e-02, -5.4457e-02, -3.4235e-02,\n",
       "        -2.4562e-02, -9.4467e-02, -3.1594e-02, -1.9532e-02, -7.8766e-02,\n",
       "        -5.5911e-02, -5.1251e-02,  6.8287e-03,  2.3901e-03,  1.7341e-02,\n",
       "        -1.8982e-02, -2.9259e-02,  3.0352e-02, -3.9701e-04,  3.0513e-03,\n",
       "        -2.7301e-02, -2.9516e-02, -5.3280e-02, -3.6616e-02, -8.8687e-03,\n",
       "         4.0112e-02,  1.5480e-02, -5.7062e-02,  4.7703e-02, -2.7718e-02,\n",
       "         9.1597e-02, -3.3947e-02,  1.3608e-02, -2.8517e-02, -4.5752e-02,\n",
       "        -6.7585e-03, -4.2150e-02, -5.9839e-02, -4.6429e-02, -1.5811e-02,\n",
       "        -5.3163e-02, -2.8281e-02,  4.4063e-02,  8.1345e-03,  5.0199e-02,\n",
       "        -5.3898e-02, -3.7279e-02, -5.6216e-02, -2.1727e-02,  3.9999e-03,\n",
       "        -6.4339e-03, -1.7183e-02, -4.4276e-02, -1.2356e-02,  1.2241e-02,\n",
       "        -1.3105e-02, -1.0988e-02, -2.2333e-02,  4.3900e-03,  6.5215e-02,\n",
       "         1.5148e-02, -3.6541e-02, -2.8720e-02,  1.6120e-05, -8.2765e-02,\n",
       "         9.5096e-03, -2.2613e-02, -8.7813e-02, -5.6625e-02, -1.4889e-02,\n",
       "         8.6411e-03, -2.5157e-02,  8.4695e-03, -1.0625e-02, -3.1585e-02,\n",
       "         7.2671e-02, -4.4576e-02,  6.5375e-02, -5.8678e-02, -6.8635e-02,\n",
       "        -4.9255e-03,  3.0885e-02, -4.0078e-02, -1.1824e-02, -3.9481e-02,\n",
       "        -3.7534e-02, -2.8148e-02, -6.7783e-02, -4.2159e-02,  7.4410e-04,\n",
       "        -3.1102e-02,  9.7771e-03, -6.2143e-02, -1.6781e-02,  4.5281e-02,\n",
       "         2.0445e-02,  1.1449e-02,  9.8312e-05, -5.3495e-02, -5.9770e-02,\n",
       "        -1.2183e-02, -1.5877e-02,  4.3193e-02, -8.7390e-02,  3.0446e-02,\n",
       "        -4.0469e-03,  6.6696e-03,  2.5148e-02, -9.2288e-03,  4.7478e-02,\n",
       "         1.4913e-03, -1.8715e-02,  1.2482e-02,  4.2584e-03, -4.4926e-02,\n",
       "         2.9420e-02,  3.6180e-02, -1.0555e-02, -2.7127e-02,  1.3765e-02,\n",
       "         6.8694e-03, -6.9724e-02,  1.4761e-02, -3.2281e-02, -6.4907e-02,\n",
       "         1.2463e-02, -1.8050e-02, -6.7536e-03, -2.2448e-02, -2.7862e-02,\n",
       "        -2.7647e-02,  7.2892e-02,  1.8628e-03, -4.6115e-02, -5.6270e-02,\n",
       "        -3.1500e-02, -4.7854e-02, -4.5692e-02, -2.0432e-02, -4.4274e-02,\n",
       "        -8.8417e-03, -2.1905e-02,  3.6729e-02,  3.7687e-02, -2.4796e-03,\n",
       "         1.0819e-03, -3.2691e-02, -9.0253e-02, -3.0312e-03,  4.6084e-02,\n",
       "         2.0369e-02, -7.0231e-02, -2.7333e-02, -8.7663e-02,  8.5675e-03,\n",
       "        -2.2886e-03, -4.8087e-02,  2.1552e-02,  1.4463e-02, -1.6634e-03,\n",
       "        -3.7142e-03,  2.1196e-02,  1.4076e-02,  1.5722e-02,  2.6415e-02,\n",
       "         2.6921e-02, -1.9087e-02, -1.5705e-02, -2.0968e-03,  4.8123e-02,\n",
       "        -5.7070e-02, -2.8769e-02, -4.2321e-03,  9.9865e-03, -1.7661e-02,\n",
       "        -3.1574e-02,  7.2262e-03,  3.2572e-02, -1.0170e-01, -2.3048e-02,\n",
       "         2.4829e-02,  2.5639e-02, -8.7369e-03, -6.6024e-03, -3.4674e-02,\n",
       "        -1.1234e-02,  4.3441e-02,  1.6480e-02, -1.9962e-02, -5.5474e-02,\n",
       "         3.4495e-02,  1.7296e-02,  1.4537e-02,  1.4691e-02, -3.5560e-02,\n",
       "         1.0574e-02,  2.0935e-02, -1.1333e-02, -4.3980e-02, -1.5276e-02,\n",
       "        -1.7890e-02,  5.8147e-03,  3.8492e-02, -2.8828e-02,  4.0850e-02,\n",
       "        -5.2343e-02, -4.2216e-02,  3.3646e-02,  5.4906e-03,  1.7957e-02,\n",
       "        -1.8939e-02,  4.5502e-02, -1.5830e-02,  7.4741e-02, -1.3465e-02,\n",
       "        -1.9749e-02,  4.5204e-02,  5.2851e-02, -3.6431e-02,  2.3607e-02,\n",
       "        -2.8735e-02, -4.4660e-02,  7.7867e-02, -5.3180e-02, -4.4169e-02,\n",
       "        -1.9887e-02, -1.3428e-02,  5.7163e-02, -3.0353e-02, -8.1256e-03,\n",
       "         1.9528e-02, -9.0187e-03, -6.1055e-02, -4.0301e-02,  2.1307e-02,\n",
       "        -2.1109e-03, -5.1682e-02,  3.5288e-02, -1.1608e-02, -1.1204e-02,\n",
       "         1.0502e-02, -4.7630e-02,  2.7808e-03,  5.8654e-02,  7.8917e-03,\n",
       "         2.4163e-02,  1.7546e-02,  6.3508e-03, -4.6726e-02,  1.7979e-02,\n",
       "         8.0103e-03, -5.7780e-02,  2.5366e-02,  2.2173e-02,  3.1421e-02,\n",
       "         1.1779e-02,  6.2488e-03, -1.0969e-02,  1.2830e-02, -3.2279e-02,\n",
       "        -4.2476e-02, -5.0352e-02, -6.9797e-03,  6.5312e-02, -5.9164e-02,\n",
       "         4.3400e-02,  5.4692e-03, -1.8442e-02,  8.8275e-03, -9.5528e-03,\n",
       "        -4.3329e-02, -6.2202e-02, -2.0282e-02, -5.0583e-02, -7.7592e-03,\n",
       "        -3.5848e-02, -1.4712e-02,  5.9468e-02, -6.7132e-02, -4.5774e-02,\n",
       "        -1.6080e-01, -2.0764e-03, -1.1149e-01,  2.5021e-03, -2.6869e-02,\n",
       "        -1.1405e-02, -3.7546e-02, -3.8675e-02,  1.7745e-02,  5.7616e-03,\n",
       "        -5.5724e-02, -6.6041e-02, -9.0923e-03, -9.3419e-02, -9.3210e-03,\n",
       "        -7.2311e-02, -9.5221e-02,  6.0981e-02, -1.3484e-02, -3.4046e-02,\n",
       "        -3.1859e-03,  6.5745e-03, -5.2395e-02, -4.4301e-02, -5.4276e-02,\n",
       "         3.6095e-02, -4.1105e-02,  2.1378e-02,  2.4527e-02, -2.3856e-02,\n",
       "         2.1367e-02, -9.4081e-03,  5.9979e-02,  1.2796e-02, -1.1280e-01,\n",
       "        -5.2191e-02,  1.0212e-03,  6.6099e-03, -5.4558e-03, -1.5545e-02,\n",
       "        -2.0038e-02,  2.7379e-02, -6.4059e-02,  2.5964e-02, -3.4578e-02,\n",
       "         1.9719e-02, -2.0732e-02, -1.5417e-02])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['bert.encoder.layer.11.attention.self.query.weight'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(self_generated_dict, 'reinitialize_weights_layer0_layer1.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_state_dict = torch.load('reinitialize_weights_layer0_layer1.bin', map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "torch.Size([30522, 768])\n",
      "bert.embeddings.position_embeddings.weight\n",
      "torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.embeddings.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.pooler.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.pooler.dense.bias\n",
      "torch.Size([768])\n",
      "cls.predictions.bias\n",
      "torch.Size([30522])\n",
      "cls.predictions.transform.dense.weight\n",
      "torch.Size([768, 768])\n",
      "cls.predictions.transform.dense.bias\n",
      "torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "cls.predictions.decoder.weight\n",
      "torch.Size([30522, 768])\n",
      "cls.seq_relationship.weight\n",
      "torch.Size([2, 768])\n",
      "cls.seq_relationship.bias\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# for k in new_state_dict.keys():\n",
    "#     print(k)\n",
    "#     print(new_state_dict[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self_generated_dict['bert.encoder.layer.0.attention.self.query.weight'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# self_generated_dict['bert.encoder.layer.0.attention.self.query.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dict['bert.encoder.layer.0.attention.self.query.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new_state_dict['bert.encoder.layer.0.attention.self.query.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
