{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#archive_file=\"../pretrained/bert-base-uncased-pytorch_model.bin\"\n",
    "archive_file=\"bert-base-uncased-pytorch_model.bin\"\n",
    "state_dict = torch.load(archive_file, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "torch.Size([30522, 768])\n",
      "bert.embeddings.position_embeddings.weight\n",
      "torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.embeddings.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.pooler.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.pooler.dense.bias\n",
      "torch.Size([768])\n",
      "cls.predictions.bias\n",
      "torch.Size([30522])\n",
      "cls.predictions.transform.dense.weight\n",
      "torch.Size([768, 768])\n",
      "cls.predictions.transform.dense.bias\n",
      "torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "cls.predictions.decoder.weight\n",
      "torch.Size([30522, 768])\n",
      "cls.seq_relationship.weight\n",
      "torch.Size([2, 768])\n",
      "cls.seq_relationship.bias\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# for k in state_dict.keys():\n",
    "#     print(k)\n",
    "#     print(state_dict[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self_generated_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import truncnorm\n",
    "\n",
    "\n",
    "\n",
    "def truncated_normal(size, threshold=1):\n",
    "    values = truncnorm.rvs(-threshold, threshold, size=size)\n",
    "    x = torch.from_numpy(values)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain_embedding_dict = {}\n",
    "# for k in state_dict.keys():\n",
    "#     if 'embeddings' in k:\n",
    "#         pretrain_embedding_dict[k] = state_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(pretrain_embedding_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self_generated_dict = {}\n",
    "# for k in state_dict.keys():\n",
    "#     print(k)\n",
    "#     print(state_dict[k].shape)\n",
    "#     parameters = truncated_normal(state_dict[k].shape)\n",
    "#     self_generated_dict[k] = parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "torch.Size([30522, 768])\n",
      "bert.embeddings.position_embeddings.weight\n",
      "torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.embeddings.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.pooler.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.pooler.dense.bias\n",
      "torch.Size([768])\n",
      "cls.predictions.bias\n",
      "torch.Size([30522])\n",
      "cls.predictions.transform.dense.weight\n",
      "torch.Size([768, 768])\n",
      "cls.predictions.transform.dense.bias\n",
      "torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "cls.predictions.decoder.weight\n",
      "torch.Size([30522, 768])\n",
      "cls.seq_relationship.weight\n",
      "torch.Size([2, 768])\n",
      "cls.seq_relationship.bias\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "self_generated_dict = {}\n",
    "for k in state_dict.keys():\n",
    "    print(k)\n",
    "    print(state_dict[k].shape)\n",
    "    if 'embeddings' in k:\n",
    "        self_generated_dict[k] = state_dict[k]\n",
    "    else:\n",
    "        parameters = truncated_normal(state_dict[k].shape)\n",
    "        self_generated_dict[k] = parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(self_generated_dict, 'reinitialize_weights_except_embedding.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = torch.load('reinitialize_weights_except_embedding.bin', map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "torch.Size([30522, 768])\n",
      "bert.embeddings.position_embeddings.weight\n",
      "torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.embeddings.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "bert.pooler.dense.weight\n",
      "torch.Size([768, 768])\n",
      "bert.pooler.dense.bias\n",
      "torch.Size([768])\n",
      "cls.predictions.bias\n",
      "torch.Size([30522])\n",
      "cls.predictions.transform.dense.weight\n",
      "torch.Size([768, 768])\n",
      "cls.predictions.transform.dense.bias\n",
      "torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.gamma\n",
      "torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.beta\n",
      "torch.Size([768])\n",
      "cls.predictions.decoder.weight\n",
      "torch.Size([30522, 768])\n",
      "cls.seq_relationship.weight\n",
      "torch.Size([2, 768])\n",
      "cls.seq_relationship.bias\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for k in new_state_dict.keys():\n",
    "    print(k)\n",
    "    print(new_state_dict[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self_generated_dict['bert.encoder.layer.0.attention.self.query.weight'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_generated_dict['bert.encoder.layer.0.attention.self.query.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['bert.encoder.layer.0.attention.self.query.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state_dict['bert.encoder.layer.0.attention.self.query.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2061e-02, -6.4849e-02,  1.0310e-02, -1.6152e-02,  2.6366e-02,\n",
       "         6.9367e-03,  3.8524e-02,  1.8418e-02,  3.7026e-02, -1.8532e-02,\n",
       "         1.0845e-02,  1.8458e-02,  3.6670e-02, -1.3521e-02, -8.6471e-03,\n",
       "         8.9971e-03,  1.3636e-02, -4.3722e-02,  4.3119e-02, -1.6299e-02,\n",
       "         9.0398e-03, -1.8695e-02,  1.7301e-02, -3.3607e-02,  1.4717e-03,\n",
       "        -1.1895e-02, -5.6709e-03, -6.3327e-02,  2.1326e-02,  3.2670e-02,\n",
       "         5.9227e-03, -2.0862e-02, -1.1523e-02, -3.9250e-02, -3.7531e-03,\n",
       "         1.0343e-02, -7.3910e-03, -8.6001e-03, -4.1540e-03, -6.0079e-02,\n",
       "         1.6942e-02, -1.9861e-03,  6.3997e-03,  8.6872e-03, -2.5488e-02,\n",
       "        -3.9130e-02,  1.6253e-02, -3.6469e-02, -7.4835e-02,  2.4417e-02,\n",
       "         5.2933e-02, -5.5506e-02, -8.1154e-04,  8.2942e-02, -1.3503e-02,\n",
       "        -1.1108e-02, -2.0516e-02,  6.8484e-03, -7.2455e-02,  7.5639e-02,\n",
       "        -2.7543e-02,  2.9131e-02,  4.9544e-02, -5.8123e-02,  8.9478e-03,\n",
       "         7.5850e-03, -4.5344e-02,  2.8186e-02,  3.2647e-02, -5.9080e-02,\n",
       "        -1.2127e-02, -2.8229e-02,  5.2056e-03,  5.9008e-02,  5.1244e-02,\n",
       "         1.3278e-02,  3.4846e-03,  2.5538e-02,  4.0654e-02, -1.9557e-02,\n",
       "         1.0085e-02,  2.5263e-02,  5.2007e-02, -4.5059e-02,  4.5973e-03,\n",
       "        -2.5917e-02, -2.9722e-02,  8.8995e-03, -3.8060e-02,  1.0400e-02,\n",
       "         1.8195e-02,  2.0556e-02, -7.3521e-02,  3.8102e-03, -1.1907e-02,\n",
       "        -5.2312e-02,  5.7401e-02,  4.3469e-02, -8.6951e-03, -3.6678e-02,\n",
       "         1.9572e-02,  4.7053e-02,  1.0759e-03,  1.7169e-02, -8.9376e-03,\n",
       "         5.7249e-02, -1.3279e-03, -2.4719e-02, -2.6578e-02, -4.6775e-03,\n",
       "         9.8569e-03, -3.4079e-02,  6.0333e-03, -1.2260e-02, -1.3879e-03,\n",
       "        -3.9514e-02, -3.2153e-02, -4.2344e-02, -3.1911e-02,  6.2699e-02,\n",
       "        -3.0516e-02,  1.5836e-03, -4.6311e-02,  5.0857e-02, -3.7904e-02,\n",
       "        -6.1037e-03, -2.7961e-03,  6.4547e-03, -6.7263e-02,  5.7465e-02,\n",
       "         8.2827e-03, -3.1828e-02, -8.2899e-02,  9.9426e-03, -3.9688e-02,\n",
       "        -2.0658e-02,  3.6214e-02,  4.2451e-02,  1.4101e-02, -6.2890e-03,\n",
       "         4.5336e-02, -2.0043e-02,  7.9951e-03, -3.7104e-02, -5.1501e-02,\n",
       "         3.2964e-02, -1.5902e-02, -2.8920e-02, -3.8520e-03, -3.5447e-02,\n",
       "         4.0087e-02,  1.5944e-03, -7.9592e-02,  3.7207e-02, -3.0321e-02,\n",
       "         2.1330e-02,  7.9912e-03,  1.3319e-02, -1.1404e-02, -9.0757e-03,\n",
       "         9.0673e-02, -2.4931e-02, -3.2649e-02,  1.3826e-02,  9.8205e-03,\n",
       "         2.6106e-02, -2.1788e-03, -4.0001e-02,  6.9144e-02,  5.4846e-02,\n",
       "         4.0605e-02,  9.8013e-03,  2.2525e-02,  1.3486e-02, -3.3249e-02,\n",
       "        -1.5052e-02,  2.0143e-02,  6.2744e-03,  3.1981e-02,  9.3554e-03,\n",
       "        -4.3254e-03, -4.3058e-03,  1.4697e-04,  1.8783e-02,  6.4920e-02,\n",
       "         4.5283e-03,  7.4183e-03,  2.9268e-02, -1.6977e-02, -5.5316e-02,\n",
       "        -6.1888e-02,  7.0946e-02,  9.6056e-03,  2.2499e-02, -3.2416e-03,\n",
       "         2.0055e-02,  5.1224e-02, -9.4499e-02,  1.7872e-02, -6.7292e-03,\n",
       "        -2.3278e-02, -3.6250e-02, -6.7856e-03,  5.6957e-02,  4.1862e-02,\n",
       "         3.5958e-02,  2.0048e-02,  7.4019e-02,  3.3465e-02,  5.7163e-02,\n",
       "         3.2349e-02, -4.4866e-02,  2.6980e-02, -6.3754e-03,  2.9747e-02,\n",
       "        -1.0485e-01,  1.0309e-01,  9.9057e-03, -3.9740e-02, -4.8300e-03,\n",
       "         1.9736e-02,  3.4555e-02,  5.0952e-02, -9.3914e-03, -5.5515e-02,\n",
       "        -1.5517e-01,  3.1392e-02, -1.1537e-02, -9.0401e-02, -1.5076e-02,\n",
       "        -4.8702e-04,  1.1185e-04, -4.9310e-02, -3.4760e-02, -2.6229e-02,\n",
       "        -4.8032e-02,  3.7840e-02, -1.4120e-02,  2.2360e-02,  8.1173e-03,\n",
       "         4.2120e-02,  4.2124e-02, -4.0663e-02, -7.2436e-03, -2.4377e-04,\n",
       "         3.7542e-03, -5.3251e-02, -9.0066e-03,  9.4398e-03, -5.9184e-02,\n",
       "        -1.2988e-02,  1.0128e-02, -7.5271e-03, -2.4943e-02, -8.6653e-03,\n",
       "         4.2702e-02, -1.1168e-02, -4.6904e-02,  1.0180e-02, -1.8213e-02,\n",
       "         7.8245e-03,  4.8054e-02, -3.0473e-02, -3.5735e-02,  7.2757e-02,\n",
       "        -7.9364e-03,  3.6673e-02,  1.4940e-02, -3.3802e-02, -3.4780e-02,\n",
       "         3.0085e-02, -6.7709e-03,  4.4932e-02,  4.6757e-03, -2.9788e-03,\n",
       "         1.8478e-02,  4.3936e-02, -6.6406e-03, -1.6862e-02, -3.2222e-02,\n",
       "         1.3179e-02, -2.7834e-03, -2.4301e-02,  7.6324e-02, -1.2763e-02,\n",
       "        -2.9237e-02, -1.9443e-03, -1.8534e-03,  3.5250e-02,  2.1730e-02,\n",
       "        -2.0110e-02, -1.2053e-02,  8.9827e-02,  9.5188e-02,  2.5828e-02,\n",
       "        -2.5490e-02, -2.9193e-02, -5.1941e-02, -3.9961e-03,  3.1302e-02,\n",
       "         5.4580e-02, -5.2214e-02, -5.9840e-02, -9.9751e-03,  1.4925e-02,\n",
       "         3.3093e-02, -1.0322e-02, -8.5508e-03,  3.2394e-02,  1.4378e-02,\n",
       "         3.4140e-02, -5.6094e-02,  2.3395e-02,  2.0315e-03,  6.0065e-02,\n",
       "         1.3142e-02, -5.7472e-03, -3.2858e-02, -4.7392e-03,  5.6038e-02,\n",
       "        -3.4224e-02,  3.0486e-02, -3.0558e-03, -1.6374e-02,  8.1856e-03,\n",
       "        -2.2509e-02,  1.2120e-01,  2.8454e-02,  1.9569e-03, -2.6268e-03,\n",
       "        -2.1934e-02, -1.2028e-02,  6.5664e-02,  3.3652e-03,  3.9332e-02,\n",
       "         2.5454e-02, -1.4775e-02, -2.1606e-02, -1.3233e-02, -2.5885e-02,\n",
       "        -4.8101e-02, -1.5003e-02, -2.6694e-02, -1.2438e-03, -4.8510e-02,\n",
       "         2.3745e-02, -1.9321e-02,  5.4568e-03,  2.1475e-02,  2.0110e-02,\n",
       "         1.5950e-02, -3.7506e-02,  1.8782e-02,  1.0246e-01, -1.7170e-02,\n",
       "        -1.9865e-03, -3.8723e-02, -1.4368e-02, -8.1732e-02,  3.2039e-02,\n",
       "         1.4286e-02,  1.2137e-02, -2.9275e-03, -6.6747e-03,  1.9390e-02,\n",
       "        -9.5044e-03,  5.5218e-02, -4.1482e-02,  3.2219e-02, -1.8810e-02,\n",
       "        -3.2651e-02,  4.1346e-03, -1.9566e-02, -1.0407e-02,  2.8659e-02,\n",
       "         3.7229e-02,  1.9862e-02,  1.1391e-02, -1.2935e-02,  2.6845e-02,\n",
       "         4.1706e-02, -1.8180e-02,  2.3155e-03,  4.5938e-02,  8.0737e-03,\n",
       "         3.0862e-02, -3.1914e-02,  3.7021e-03, -8.9181e-05,  8.9865e-02,\n",
       "         3.7265e-02,  1.0770e-02, -2.0516e-02,  6.7076e-03,  2.7077e-02,\n",
       "        -5.0545e-02, -1.6998e-02, -4.9444e-02,  1.3224e-02, -1.4326e-02,\n",
       "         2.4916e-02,  2.6297e-02,  2.4247e-03, -4.3576e-03, -6.9065e-02,\n",
       "        -1.2815e-02, -2.2646e-03, -1.8873e-02,  2.9400e-02, -6.0980e-02,\n",
       "         4.3410e-02,  1.0357e-02, -5.7400e-02, -5.7975e-02,  3.7450e-04,\n",
       "         2.2487e-03, -9.1816e-03, -8.4681e-03, -1.0133e-02, -9.8659e-03,\n",
       "         1.1376e-03, -4.6781e-03, -4.7431e-03, -6.7715e-03,  5.5026e-02,\n",
       "        -2.8531e-02, -2.1704e-02,  7.0125e-02, -1.2804e-02, -4.6231e-02,\n",
       "        -2.8591e-02, -1.2808e-03,  1.8432e-02,  8.9597e-02,  8.1131e-03,\n",
       "         8.7803e-03, -3.6477e-03,  2.9129e-02, -1.9354e-02, -5.9063e-03,\n",
       "        -3.3469e-02, -1.0727e-02, -4.2662e-02,  1.0304e-02,  3.0732e-02,\n",
       "        -3.4515e-03, -6.1079e-03,  7.7513e-02,  9.6115e-03,  8.4953e-03,\n",
       "         3.8172e-02,  2.7347e-02,  1.5754e-02,  6.4504e-03, -7.7689e-05,\n",
       "         6.0105e-02,  3.8466e-02, -1.5618e-02, -3.0510e-03,  2.6934e-02,\n",
       "        -2.2135e-02, -7.5292e-02, -1.8013e-02, -6.7652e-02, -7.3154e-02,\n",
       "        -2.2442e-02, -2.4875e-02, -7.6978e-04, -3.9754e-02, -2.8407e-02,\n",
       "        -2.7432e-02,  1.9285e-03,  7.8394e-03,  6.3099e-03, -1.0832e-02,\n",
       "        -4.9584e-02, -2.8186e-02, -5.0716e-03, -5.6434e-02, -5.5179e-04,\n",
       "        -1.3295e-02, -2.3392e-03,  1.3425e-02,  6.9169e-02,  1.7700e-02,\n",
       "        -3.5281e-02, -4.5990e-02, -2.2212e-02, -2.9102e-02, -4.3563e-03,\n",
       "         1.9190e-02,  2.2421e-02,  4.6755e-02,  1.7954e-02, -1.4772e-02,\n",
       "        -8.1620e-03, -2.3143e-02, -2.1699e-02, -1.7660e-02,  5.3253e-02,\n",
       "         7.6241e-02,  5.2871e-02,  9.8680e-04, -3.0279e-02, -7.0409e-02,\n",
       "        -1.8343e-02,  2.8764e-02,  1.9612e-02, -4.3584e-02,  1.0022e-03,\n",
       "        -8.7254e-04,  7.2042e-02,  5.3466e-02, -4.9847e-02,  4.7905e-02,\n",
       "        -5.8723e-02, -2.2894e-02,  7.0549e-02,  5.2072e-03, -1.4977e-02,\n",
       "         2.4213e-02, -7.2436e-03, -3.8520e-02,  2.5956e-02, -4.4106e-03,\n",
       "         4.4673e-02, -5.7072e-02,  4.7930e-02, -2.0713e-02, -2.5920e-02,\n",
       "        -2.4784e-02, -2.4511e-02, -6.6002e-02, -2.0194e-02, -3.9647e-02,\n",
       "         2.3800e-02,  2.9039e-02, -3.9807e-02,  3.5558e-02, -4.8066e-02,\n",
       "         1.4285e-02,  1.7970e-02, -6.0712e-02,  3.6301e-03, -1.0208e-02,\n",
       "        -9.3383e-03,  4.8971e-02,  3.3020e-02,  1.8048e-02,  4.5180e-02,\n",
       "         3.8733e-02, -2.9535e-02, -4.9685e-02,  3.2499e-02, -1.4811e-02,\n",
       "        -9.2152e-03, -4.0630e-02,  3.5939e-02, -8.4060e-04, -1.4360e-02,\n",
       "         1.5763e-02,  1.7881e-02, -3.3634e-02, -1.5213e-02,  1.7470e-02,\n",
       "        -2.9747e-03,  2.2235e-02, -2.7657e-02, -4.0760e-02, -1.2496e-02,\n",
       "        -3.3724e-02, -5.5939e-02, -4.9204e-02, -3.9912e-02,  2.9182e-02,\n",
       "         7.1391e-03, -1.9203e-02, -1.0841e-01,  1.3910e-02, -4.6966e-02,\n",
       "         1.0504e-03, -2.5583e-02, -2.1266e-02, -4.4574e-02, -4.3359e-02,\n",
       "        -1.6125e-02,  5.9915e-02, -4.0039e-02,  3.7308e-02, -3.1430e-02,\n",
       "        -1.4796e-03, -7.8099e-02,  1.9650e-02,  3.2812e-02, -1.6749e-02,\n",
       "        -1.0356e-02,  5.7137e-02, -2.5531e-02, -2.4160e-02, -4.4201e-02,\n",
       "        -4.6581e-02,  1.3177e-03, -2.1579e-03,  4.6374e-03, -1.7493e-02,\n",
       "         3.0406e-03,  1.9329e-02,  3.3268e-02,  4.3392e-02,  1.4723e-03,\n",
       "         4.6892e-02,  3.7312e-02, -2.7528e-02,  3.4775e-02,  3.7749e-02,\n",
       "         3.8271e-02, -6.8823e-03,  2.1137e-02, -2.0113e-03, -8.0825e-03,\n",
       "         8.5287e-03,  2.3461e-02,  4.3183e-02,  2.4988e-02, -3.7304e-03,\n",
       "        -5.8860e-02, -2.0113e-02, -3.0327e-02, -2.3310e-02, -1.2593e-02,\n",
       "        -2.6385e-02, -1.3876e-02,  1.6644e-03, -7.7515e-02, -2.7219e-02,\n",
       "        -2.5627e-02,  6.0566e-02, -1.6707e-02, -4.0913e-02,  2.6096e-02,\n",
       "         1.9248e-02, -7.4249e-04, -7.3871e-02,  3.8401e-02,  5.7489e-02,\n",
       "        -3.5244e-02,  3.8690e-02, -5.1081e-03,  3.9829e-02, -6.3185e-02,\n",
       "        -5.1577e-02,  1.7776e-02,  2.8196e-02,  1.7000e-02, -1.6135e-02,\n",
       "         3.7391e-02, -7.9954e-02, -1.3566e-02,  2.2089e-02,  7.1340e-03,\n",
       "        -2.6943e-02,  1.1085e-02, -9.1700e-03,  1.1012e-02,  2.0631e-02,\n",
       "        -5.8604e-02, -1.1533e-02,  2.8304e-02,  1.8814e-02,  1.6655e-02,\n",
       "        -9.9669e-02, -2.7231e-02,  8.1974e-03, -2.8236e-02,  5.9895e-02,\n",
       "         1.5747e-02,  6.3741e-02,  6.3161e-02,  2.9921e-02, -3.9068e-02,\n",
       "         3.9703e-02,  3.0427e-02,  3.5890e-02, -1.0100e-02,  2.0735e-02,\n",
       "        -7.9448e-02,  1.8513e-02,  5.0497e-03,  4.6251e-02, -1.2223e-02,\n",
       "         2.9773e-02,  3.2624e-02, -3.2591e-02, -4.1258e-02,  1.6791e-02,\n",
       "        -1.4540e-02, -6.0412e-04,  2.1516e-02,  6.4108e-02, -2.3813e-02,\n",
       "        -7.5787e-04,  1.5865e-02,  7.1860e-03,  3.4221e-02, -1.0711e-03,\n",
       "         1.6449e-02, -8.6824e-03,  2.6959e-02,  3.6236e-02,  4.0018e-02,\n",
       "         1.2639e-02,  2.3173e-02,  2.9696e-02, -2.9452e-03,  1.8179e-02,\n",
       "         1.3015e-02,  6.1585e-02, -1.0059e-02,  5.7798e-03,  5.8057e-02,\n",
       "         3.6712e-02,  9.9823e-03, -5.1776e-02, -4.2175e-02, -2.6557e-02,\n",
       "        -2.0565e-02,  5.1123e-04, -3.7794e-02,  3.3051e-02,  4.1721e-02,\n",
       "         3.1439e-02,  3.3807e-03,  3.2699e-02, -1.3781e-02, -1.6840e-03,\n",
       "        -1.0290e-02,  3.6383e-02,  1.9671e-02,  1.4107e-02,  6.0708e-02,\n",
       "         6.2263e-03, -3.1357e-02,  4.9747e-02, -5.6893e-02, -2.5187e-02,\n",
       "        -3.7296e-02, -3.3278e-02,  7.4969e-02, -1.2700e-02, -3.2114e-02,\n",
       "         1.8563e-02, -3.4704e-02, -2.2011e-02,  1.8610e-02, -3.8307e-02,\n",
       "         5.3015e-02,  4.8085e-02, -4.2676e-03,  3.2111e-02,  2.7943e-02,\n",
       "         3.6290e-02,  1.4271e-02, -1.1433e-03, -4.4091e-02,  1.1149e-02,\n",
       "        -2.1761e-02, -2.8973e-02, -2.6550e-02])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dict['bert.encoder.layer.0.attention.self.query.weight'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# f = open(\"loaded_dict.txt\",\"w\")\n",
    "# f.write( str(self_generated_dict) )\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = state_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------saving the state_dict into loaded_dict.txt----------------------------\n"
     ]
    }
   ],
   "source": [
    "            print(\"-----------------------saving the state_dict into loaded_dict.txt----------------------------\")\n",
    "            f = open(\"loaded_dict.txt\",\"w\")\n",
    "            f.write(str(state_dict) )\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.gamma', 'bert.embeddings.LayerNorm.beta', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma', 'bert.encoder.layer.0.attention.output.LayerNorm.beta', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.gamma', 'bert.encoder.layer.0.output.LayerNorm.beta', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.gamma', 'bert.encoder.layer.1.attention.output.LayerNorm.beta', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.gamma', 'bert.encoder.layer.1.output.LayerNorm.beta', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.gamma', 'bert.encoder.layer.2.attention.output.LayerNorm.beta', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.gamma', 'bert.encoder.layer.2.output.LayerNorm.beta', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.gamma', 'bert.encoder.layer.3.attention.output.LayerNorm.beta', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.gamma', 'bert.encoder.layer.3.output.LayerNorm.beta', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.gamma', 'bert.encoder.layer.4.attention.output.LayerNorm.beta', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.gamma', 'bert.encoder.layer.4.output.LayerNorm.beta', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.gamma', 'bert.encoder.layer.5.attention.output.LayerNorm.beta', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.gamma', 'bert.encoder.layer.5.output.LayerNorm.beta', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.gamma', 'bert.encoder.layer.6.attention.output.LayerNorm.beta', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.gamma', 'bert.encoder.layer.6.output.LayerNorm.beta', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.gamma', 'bert.encoder.layer.7.attention.output.LayerNorm.beta', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.gamma', 'bert.encoder.layer.7.output.LayerNorm.beta', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.gamma', 'bert.encoder.layer.8.attention.output.LayerNorm.beta', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.gamma', 'bert.encoder.layer.8.output.LayerNorm.beta', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.gamma', 'bert.encoder.layer.9.attention.output.LayerNorm.beta', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.gamma', 'bert.encoder.layer.9.output.LayerNorm.beta', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.gamma', 'bert.encoder.layer.10.attention.output.LayerNorm.beta', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.gamma', 'bert.encoder.layer.10.output.LayerNorm.beta', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.gamma', 'bert.encoder.layer.11.attention.output.LayerNorm.beta', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.gamma', 'bert.encoder.layer.11.output.LayerNorm.beta', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
